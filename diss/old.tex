\chapter{Old Background and Literature Review} \label{chap:sota}

\minitoc \mtcskip \noindent

This section aims the analysis and reflection about some works and topics that will be relevant to fully understand the problem. The study of solutions found by other authors can simplify the difficult task that is the analysis of social media data.
Hence, this section has been divided into several parts in order to perceive not only the environment in which the problem is located but also the most important points to be studied in order to build our the final product. Respectively to the problem scope its important to know what is a smart city and how the transportation system can contribute to this meaning. Since our product is a framework which goal is to extract information about social media data, i.e. texts from the Twitter, its interesting obtain the knowledge about extraction tools, such as the Twitter APIs, in order to have an idea how to construct our crawler module. The meaning of text mining and how the information present in texts can be extracted through different kinds of techniques, regarding disambiguation, filtering and modeling. Finally, it is also important to analyze several works about sentiment analysis in order to know the different methodologies and which are the most advantageous for this problem.

\section{Smart Cities and Intelligent Transportation Systems}\label{sec:smartcities}

The Smart City concept appeared thanks to the continuous growth of a city's population which has contributed to an aggressive urbanization \cite{kn:Cecilia2016}. In the last few years, several definitions for its meaning have emerged, but the ideal one is not yet fully known. Angelidou in \cite{kn:Angelidou2015} defines Smart City as a "conceptual urban development model on the basis of the utilization of human, collective, and technological capital for the development of urban agglomerations" and enhance as its primary key the knowledge and the innovation economy. In her work, there is an identification of four forces that model the concept of a Smart City and two of them are very important to enhance: \textit{technology push}, where new products and solutions are introduced in the market regarding the fast advance in science and technology; \textit{demand pull}, where solutions and problems are developed in order to respond to the society demands, like the continuous growth of the population \cite{kn:Angelidou2015}.

The development environment in a city tagged with the concept "smart" is another key factor to reach the success. Komninos focus the importance of collective sources of innovation to the improvement of life quality in cities. The globalization of innovation networks are the responsible for the emergency of another types of environments, such as "global "innovation clusters and i-hubs, intelligent agglomerations, intelligent technology districts and intelligent clusters, living labs" making possible the experimentation of products or services by the population in order to identify problems or even to analyse the behavior of the people regarding what they have experimented \cite{kn:Komninos2009}.

The transportation system is inherently connected to the progress of a city, since people on a daily-basis uses the several ways of transportation, i.e. bus, private cars, metropolitan, etc, to go to their jobs and make their own life. This system is also influenced by the problem of the population growth being relevant the need of finding solutions to minimize or even raze it \cite{kn:Caragliu2015}. Hence, "a smart city should be focused on its actions to become smart", coming up the concept of innovation \cite{kn:Cecilia2016}.

To understand what are \textit{Intelligent Transportation Systems}, it is crucial introduce the meaning of Smart Mobility. SM is a combination of comprehensive and smarter traffic service with smart technology, enabling several intelligent traffic systems which provide control in the signals regarding the traffic volume, information about smooth traffic flows, times of bus, train, subway and flight arrivals and their routes \cite{kn:Chun2015}. The majority of \textit{Intelligent Transportation Systems} are expressed through smart applications where the transportation and traffic management has became more efficient and practicable, allowing the users to access important information about the transportation systems in order to make correct decisions about what they want to use in their cities \cite{kn:Caragliu2015}. ICT-based infrastructures are the main support for Smart Cities when the focus are ITS, since through that is possible to pilot the activities operations and its management over a long period of time \cite{kn:Cecilia2016}.

Nowadays, cities are exploring some initiatives of sensing to support the development of technological projects. Areas such as utilities management (where, for example, is monitored the consumption level of power, water and gas), traffic management (using vibration sensors to measure the traffic flows on bridges, or even the full capacity of a parking lot), environment awareness (using video cameras to monitor the population behaviour and sensors to measure the level of air pollution) make use of physical sensors, i.e. some devices that can capture information to study and improve the quality of life in a daily basis \cite{kn:Doran2015}. R. Szabo et al. \cite{kn:Szabo2013} and D. Doran et al. \cite{kn:Doran2015} report the highly economic cost that this kind of sensing needs, since it's require the maintenance and replacement of this devices, as well as a tracking infrastructure store and treat the information collected. Hence, a new form of sensing has emerged - Crowd Sensing - to offer the cities several ways to improve their services by exploring the participation of the population in the social networks where there are a publicly share of citizen's opinions and thoughts regarding some problems \cite{kn:Roitman2012}. This type of sensing consists in \textit{human-generated} data provided by the population through the use of mobile devices and the social networks platforms. Such data can be further used to extract some analytics regarding specific services in a city, namely the urban transportation system \cite{kn:Roitman2012}. Based on all this, social media can be seen as a good source of data to extract valuable information in order to direct it to the smartness evolution process of a city \cite{kn:Szabo2013}. 

Several works have already been developed and presented taking into account these two large areas, Transportation Services and Smart Cities, using social media as source of information.
G. Anastasi et al. \cite{kn:Anastasi2013} proposed a framework which objective was the promotion of flexible transportation systems usage, i.e. encouraging people to share transport or to opt for the use of bicycles in order to minimize infrastructural and environmental problems. Their tool takes advantages of the crowd sensing techniques by exploring social media streams to predict accidents or traffic congestion and alert the users of their service about this type of events.
W. Liu et al. \cite{kn:Liu2012} have made a study in three different transportation modes (private cars, public transportations and bicyclists) using theirs channels on Twitter to estimate a percentage of the majority gender that uses this services in the city of Toronto. They have extract all the channel's tweets appealing only to the \textit{non-protected} followers and applied an already developed classification model to label each tweet with its creator gender: male or female.

T. Ludwig et al. \cite{kn:Ludwig2015} proposed a tool capable of collect and display social media streams in order to help the integration and coordination of volunteers in actions performed by emergency services to prevent engagement in dangerous areas. Their tool present to the end-users map visualization of a city where they could identify public calls of the emergency services to accept or deny them.

In conclusion to everything that has been analyzed in this sub-section, it's possible to verify that the cities are increasingly opting for technological opportunities that involve crowd sensing, once this type of exploration brings a considerable reduction of costs and the information that is collected may contribute to the extraction of value from data generated by the population itself.

\section{Social Media Analytics}\label{sec:socialmedia}
In the last few years social networks have made impact on the business communications, since users assume the role of costumers through the publication of content on this networks, rising the levels of interaction between users and businesses entities \cite{kn:Cecilia2016}. A proof of that is the amount of information produced since 2011 which is equivalent to a number over than 90\% of the available data online \cite{kn:SINTEF}. Facebook, Twitter and other social networking sites are nowadays used as business tools by companies aiming the efficient use of digital marketing techniques to publicize their products \cite{kn:Royle2014}. Besides the business field, the population turn into this new communication technologies in a intensely way, where they publicy share real-life events, their opinions about certain topic, their on-time feelings in the network through a simple message \cite{kn:DAndrea2015}.
Social Media Analytics can be describe as a type of digital analytics to study the people interaction with others, or their opinion about companies, its products and  services through the social media data. This study provides important information to "analysts, brands, agencies or vendors", and its analysis could facilitate the generation of economic value to many organizations \cite{kn:Judah2012}.
To achieve the main goals of the SMA, the companies focus their effort in the development and evaluation of frameworks, to make possible an easy collection, analyse, summarization and visualization of processed social media data. Hence, the companies can establish specific points about what to improve in their products \cite{kn:Zeng2010}.
To create a significantly value regarding the SMA, J. Philips in \cite{kn:Judah2012} enhance some important factors: users permissions, the listening of real-time information, the search mechanism, the data access and integration, and others, before the choice of a tool that allows the information collection. Besides the tool, is also important have an idea of what is need to explore because the use of a wrong technique of SMA could have bad business impact for the company. The majority of SMA techniques focus on modeling in order to understand the large range of data collected and support techniques, such as sentiment analysis, trend analysis and topic modeling, are the most commonly used \cite{kn:Fan2013}.

\subsection{Twitter}
Twitter is a social network where people freely micro-blogging about any topic and, like any other social network, makes possible the connection between users around the world \cite{kn:Sriram2010}. This social network has faced an exponential growth since its inception, and nowadays its users, which surpass 200 millions, produce around 500 millions tweets daily, performing a massive bunch of information that could be an ideal testbed for research projects on big data \cite{kn:Lipizzi2015,kn:Goonetilleke2014}. N. Banerjee et al. \cite{kn:Banerjee2012} classify Twitter as a micro-blogging service presenting three attributes that justify such characterization.

\begin{itemize}
\item \textbf{\textit{Limited Context Information}:} The length of a Twitter message never exceeds 140 characters, which, in cases of knowledge extraction, since the amount of information is short, the final results could not be the expected and none knowledge contribute is obtained.
\item \textbf{\textit{Richness of Exchange}:} Symbolizes the great diversity of posts that exists in the micro-blog network. People talk about their daily activities, have conversations with each other and shares their thoughts (moods or opinions) about a certain event or topic.
\item \textbf{\textit{High Dimensionality}:} The informality and ambiguity present in the messages and the expanse vocabulary present in any language makes a large dimension of data. The informality of the messages can be seen as the presence of spelling errors and abbreviations in its content, while the ambiguity can be the presence of words with multiple meanings.
\end{itemize}

\begin{figure}
  \centering
    \includegraphics[width=0.8\textwidth]{michael_jackson}
    \caption{Traffic in tweets per hour relating to Michael Jackson’s death by J. Sankaranarayanan \cite{kn:Sankaranarayanan2009}}
    \label{fig:michael_jackson}
\end{figure}

Twitter has not only evolved in terms of usability, but also in the purpose of its use, i.e. Twitter is not only used as personal diary for people but also represents a source of information for reporters and journalists to find potential news about real-time events \cite{kn:Sriram2010}. A good example is given by J. Sankaranarayanan et al. \cite{kn:Sankaranarayanan2009} in the period of Michael Jackson's death. The first tweet about the incident was posted 20 minutes after the call to the 911 emergency service and, nearly, two hours before the first communication on the news as it's possible to verify in the figure \ref{fig:michael_jackson}.

One of the advantages of Twitter compared to other social networks, for example, Facebook, is the easier way to access its users originated data. While Facebook does not provide private information about its users unless there are permissions to do so, or the content shared is present in public pages or groups, Twitter allows the collection of all tweets from channels or directly from people in order to be analysed in any kind of project because the user's accounts are usually public \cite{kn:Musto2015,kn:Stieglitz2013}.

Although this freedom in the collection of data, Twitter has also a an ethical perspective and a regulation must be accomplish by developers or researchers. The TOS (Twitter Terms of Service) was created in order to make known what can be done with the data and protect the users' rights \cite{kn:kelley2013}.

\section{Text Mining}
Text mining is a derived field from Data mining and aims to extract valuable information from unstructured textual data\cite{kn:He2013}. The reason why this technology is nowadays so much explored is because of the massive amount of information that is stored in text documents, such as "text files, HTML files, chat messages and emails" and it's required an automated technique that make possible the identification, extraction, management, integration and the knowledge exploration of information from texts in a efficiently and systematically way \cite{kn:He2013}. On the other hand, the social media applications also have contributed to the growth of text mining usage where companies have seen a potential path to improve their business model and increase the economic value relatively its competitors.

A. Stavrianou et al. \cite{kn:Stavrianou2007} identify text mining as an interdisciplinary field since this technology takes advantages from Data mining techniques and combines several methodologies from similar research areas, such as Categorization, Information Extraction, Information Retrieval, Topic Tracking and Concept Linkage. A common problem related to text mining is its similarity with Information Retrieval and Information Extraction which leads people to a non-differentiation between this technologies. The difference between Information Retrieval and Text mining is established in their final goal, while IR aims to find and retrieve documents that match a certain part of a text or some keywords (e.g. Google Search Engine\footnote{\url{https://google.com}}), TM tries to discovery unknown patterns in texts that can be interpreted and explain some facts or truths contained in the lexical \cite{kn:Stavrianou2007,kn:He2013,kn:Hotho2005}. Regarding the Information Extraction, the differentiation can be seen in the data specificity and structure. IE focus on the extraction of expected information from structured data and precocious relations, while the information returned by TM techniques should be unsuspected and unexpected with the data holding an unstructured format \cite{kn:Stavrianou2007}.

The motivation behind text mining holds on the benefit that other fields of research could take from a use of its techniques. Information Retrieval systems can improve their precision since its basis is the identification of semantic relations. Several areas can explore this methodology to find inconsistencies in relational databases and make the integration, update and querying tasks easier \cite{kn:Stavrianou2007}.

Text mining shares some of the issues presented by the Natural Language Processing field. Once texts are usually performed by humans some associated problems can appear, such as spelling mistakes, wrong phrasal construction, slang among other. Before the "mining" of a text, it's important to apply some pre-processing steps in order eliminating noisy data from the primary analysis process.
A. Stavrianou et al. cite this issues very well in they work and it can be seen in Table \ref{table:textminingissues}.

\begin{table}[]
\centering
\caption{Text Mining Issues by A. Stavrianou \cite{kn:Stavrianou2007}}
\label{table:textminingissues}
\begin{tabular}{ | l | p{7cm} |}
\hline \textbf{Issue}            & \textbf{Details}\\
\hline Stop list                 & Should we take into account stop words?\\ 
\hline Stemming                  & Should we reduce the words to their stems?\\ 
\hline Noisy Data                & Should the text be clear of noisy data?\\ 
\hline Word Sense Disambiguation & Should we clarify the meaning of words in a text?\\ 
\hline Tagging                   & What about data annotation and/or part of speech characteristics?\\ 
\hline Collocations              & What about compound or technical terms?\\ 
\hline Grammar / Syntax          & Should we make a syntatic or grammatical analysis? What about data dependency, anaphoric problems or scope ambiguity?\\ 
\hline Tokenization              & Should we tokenize by words or phrases and if so, how?\\ 
\hline Text Representation       & Which terms are important? Words or phrases? Nouns or adjectives? Which text model should we use? What about word order, context, and background knowledge? \\ 
\hline Automated Learning        & Should we use categorization? Which similarity measures should be applied? \\ \hline
\end{tabular}
\end{table}

The removal of words from the text can sometimes not be desirable because some sentences can lose its information or even leads to a different meaning compared with its original form. The generation of a stop list words should be a supervised task as long as little words could induce distinct results in the text classification \cite{kn:Riloff1995}.

Stemming is a task that depends mostly from the language of the text than its domain \cite{kn:Stavrianou2007}. The main goal of this technique is to reduce a word to its root to in order to help in the calculus of distances between texts, keywords or phrases, or even in the text representation.

The noisy data is derived from spelling mistakes, acronyms and abbreviations in texts and to solve this, a conversion of this terms should be done to keep a valid integrity of the data. The most commonly solution approaches involve text edit distances (Levenshtein Distance\footnote{\url{https://en.wikipedia.org/wiki/Levenshtein_distance}}) and phonetic distances measures between known words and the mispelling ones to achieve good corrections \cite{kn:Bontcheva2013} 

Word Sense Disambiguation focus on solving the meaning ambiguity present in words. Other similar field to WSD is Name Entity Disambiguation (NED) where the disambiguation target are named-entities mentions, while WSD focus on common words. WordNet\footnote{\url{https://wordnet.princeton.edu/}} is a resource very used to extinguish this ambiguity \cite{kn:Chang2016}. There are two types of disambiguation, the supervised, where the task is support by a dictionary or a thesaurus \cite{kn:Stavrianou2007}, and the unsupervised one, where the different meanings of a word are unknown and normally learning algorithms with training examples are used to achieve good results in the disambiguation task \cite{kn:Yarowsky1995}.

Tagging can be describe as the process of labeling each term of the text with a part-of-speech tag, i.e. classify each word as a noun, verb, adjective, etc \cite{kn:Hotho2005}. Collacation are very important in text mining, since this task consist in group two or more words to give the correct meaning content in the text. Collacations are usually made before the WSD task since some compound technical terms have different meaning from the individual words which composed it \cite{kn:Stavrianou2007}.

Tokenization serves to pick up all the terms presented in a text document and to achieve this it's necessary the split of the document into a stream of words implying the removal of the punctuation marks and non-text characters \cite{kn:Hotho2005}. some authors also see tokenization as a text representation form since one of the most used models to represent texts is \textit{Bag-of-words} (BoW). This model broke down texts into words and stores it in a vector being also presented the word frequency occurrence in the text. Hence, each word may represent a feature \cite{kn:Sriram2010}. Another commonly used model to represent texts is Vector Space Models that represent all the documents in a multi-dimensional space where documents are converted to vectors and each vector may be seen as a feature. This model provides some advantages since the documents can be compared with each other by performing some specific vector operations \cite{kn:Hotho2005}.

The purpose of this section is to provide the reader the definition of what is text mining, and also the identification of basic operations and steps that are necessary for the preprocessing of this type of unstructured data - texts.

\section{Information Extraction}
Information Extraction is an important field of text mining and its main goal is finding structured information from semi-structured or unstructured texts. This kind of information can range from the identification of entities, such as people, organizations and places names, to a relation between this concepts. In the sentence, \textit{"At 1976, Apple was founded by Steve Jobs and his friends"}, its possible extract information about who were the founders of Apple and what was the year of its foundation. Problems regarding the analysis of the sentence can be located on the words "Apple" and "his", and how could a machine know that "Apple" is a reference of a technological company and not a reference to a fruit, or even, that the word "his" establish a connection between "Steve Jobs" and "friends". The exploration of this kind of information constitutes a potential measure to improve computer systems, such as search engines and database management \cite{kn:Aggarwal2012}.

When the target analysis is social media data, i.e. texts from social networks, the filtering process of the data is a crucial step. It's desirable that only related-topic information should be collected in order to avoid the existence of noisy objects in the data set \cite{kn:Cecilia2016, kn:Saleiro2013}.

Information Extraction presents a set of components that can tackle this problems, such as Named Entity Recognition (NER), Named Entity Disambiguation (NED) and among others.

\subsection{Name Entity Recognition and Name Entity Disambiguation}
NER and NED are two distinct tasks that sometimes could cause some ambiguity in its purpose.

Named Entity Recognition is seen as a sub-task of Information Extraction aiming the correct labeling of words in a text in order to have knowledge of its types. The accuracy retrieve by this task is important when further steps depends on it, e.g. Relation Extraction \cite{kn:Aggarwal2012}. Gazetteers are commonly used in this task since they provide pre-processed lists of organizations, days, places and person names which can be matching against some terms we want to recognize. In some cases, this "tools" are not enough to solve the problem because their domain is very limited and some terms can not exist, implying the use of external knowledge to fill this lack \cite{kn:Ratinov2009}.

There are two main approaches to conduct this task: Ruled-based and Statistical Learning. The Ruled-based approach focus on the definition of a set of features for each token in the text to further comparing the text with a bunch of rules. The rules are composed by patterns that should trigger some labeling action in a sequence of tokens. The definition of this rules usually requires human expertise \cite{kn:Aggarwal2012}. 
The Statistical Learning approach, also known as Statistical Machine Learning, 
treats the text as a sequence of observations which are represented by a vector of features. The final goal of this approach is the assignment of a label $y_{i}$ to each observation $x_{i}$. The mapping process usually follows a BIO notation, firstly introduced to text chunking by \cite{kn:Ramshaw1999}, where the entity name could be at the beginning (B) or inside (I) of the observation and never outside (O). Patawar et al. \cite{kn:Patawar2015} enhance three types of methods to this approach.

Supervised methods, where it's characteristic the existence of a labeled training data set to train a model and then classify a set of test to measure the model performance and accuracy. Hidden Markov Model was used in \cite{kn:Bikel1999} to recognize and classify some text. In \cite{kn:Isozaki2001}, Decision Trees were combined with a simple rule generator to prove that this method could achieve similar results as Maximum-Entropy-based methods used in \cite{kn:Borthwick1999}. Support Vector Machines were used by H. Isozaki et al. \cite{kn:Isozaki2002} where they have proved that SVM models could also achieve good results for NER tasks if some analysis was carefully made in the \textit{kernel functions} and filtering methods were applied, like the removal of useless features.

Semi-supervised methods used different amount of training data, i.e. labeled examples, and test data. The test data is usually a bigger amount facing the training data. The methodology commonly applied to this approach involves the use of "bootstrapping" which is an iterative process of training the model with progressive supervised increases of the training data set until the performance starts to decrease \cite{kn:Irmak2010}.

The last type are the Unsupervised Methods, where a large amount of labeled data is necessary and it's
difficult to have this requirement. This need bases on the huge number of features required to this kind of methods. To fill this lack, a very frequent method used is the clustering where the formation of groups is made using the similarity present in the texts domain \cite{kn:Patawar2015}.

Y. Li et al. \cite{kn:Li2013} define Named Entity Disambiguation as a "process of associating an entity name mentioned in a text to an entry, representing that entity, in a knowledge base". In the last few years, NED has been a target for a considerable number of research projects. The majority methods implemented to tackle this task are focused in three main features. Y. Li et al. \cite{kn:Li2013} enhance \textit{entity popularity} as a statistical one where there is a "assumption that the most prominent entity for a given entity mention is the most probable underlying entity for that mention". At this feature, a link between the term and its Wikipedia page reference is established. The \textit{context similarity} is another feature and aims the complementarity of the \textit{entity popularity} feature. This feature centers on similarity measures between the text in analysis and the content text of the Wikipedia page. Y. Li reveals that this feature is word-dependency since it's necessary that both texts shares identical words in order to produce expected results. \textit{Topical Coherence} is the third feature and solves the emerged problem of the second one. This feature uses the Wikipedia cross-page links mechanism in order to look up for related-topics of other entities and makes a connection with the target entity in the disambiguation problem. Through this process the domain text is expanded, decreasing the word-dependency problem appeared in the second feature.

D. Spina et al. \cite{Spina2013} present two different approaches to solve the problem of ambiguity presented in texts. The first one is \textit{entity linking} and consists in the establishment of an association between the mention in the text and a entity present in a Knowledge Base. Three steps are needed to perform the linking of the entity name to the knowledge base:

\begin{itemize}
\item \textbf{Query Expansion:} Mining the Wikipedia structure and solving co-references in the document in order to enrich the query;
\item \textbf{Candidate Generation:} Construction of a list of candidate entities from the knowledge base according to the information presented in the query;
\item \textbf{Candidate Ranking:} It is the phase in which is computed some similarity measures between the query and the entities, in order to rank the candidates and select the best one.
\end{itemize}

Another approach to tackle this problem of disambiguation of entities presented by D. Spina et al. \cite{Spina2013} is the \textit{document enrichment by linking to Wikipedia Articles}. Similar to the \textit{entity linking}, this approach is also composed by three different steps and takes advantages from text representation models, such as Bag-of-words or Vector Space Models, for example.

\begin{itemize}
\item \textbf{Mention or surface form representation:} There a context definition of the target mention to disambiguate. Text representation models are build with all set of entities that are ambiguous and the unambiguous ones which are already resolved with linking to Wikipedia pages.  
\item \textbf{Candidates entities retrieval and representation:} All the candidate entities referenced by the knowledge base (e.g. Wikipedia or Freebase) and the content on the page are converted to text representation models. After this, there is extraction of some features that can range from page categories of the candidate entities or even syntactic features.
\item \textbf{Best candidate selection:} The computing of similarity and distance functions between the two text representation models, produced in the two previous steps, is made to select the best candidate. 
\end{itemize}

Some works related to NED were made in the context of micro-blogging services, such as Twitter.

At 2010, Ferragina et al. \cite{kn:Ferragina2010} developed a system capable of identity entities in short texts as, for example, tweets. Their system take advantage of the hyperlink mechanism of Wikipedia, extracting related links between pages and the anchors texts in the links. By detecting some senses present in the anchors, they try to disambiguate the ambiguous ones through a collective agreement function, i.e. a voting classification. They used the unambiguous senses to boost the selection of the ambiguous ones and have trying some pruning in the anchors set to improve the performance of the system.

Meji et al. \cite{kn:Meij2012}, similar to Ferragina et al., also explored anchors texts in Wikipedia articles. The authors have used a supervised machine learning approach to conceive a list of candidates to disambiguate each mention present in their tweets. Their strategy focus on the identification of some patterns in each tweet, such as n-grams, to further matching it with the anchor texts of the Wikipedia articles, taking also in account the hyperlink mechanism of this Knowledge Base.

Considering this works it's possible conclude that Wikipedia is a potential source to explore in order to solve mentions of entities that could lead to a ambiguous problem. 

\subsection{Content Filtering}
The content filtering is one of the most important tasks to analyze micro-blog data (e.g. Twitter). The main goal of content filtering is the classification of Twitter posts which contains an entity name, assuming the existence of a relation between the name and the content in order to erase ambiguity in the dataset \cite{kn:spina2014}. Recently, some contests related with Online Reputation Monitoring (ORM) have explored this task of filtering content. The WePS-3 \footnote{\url{http://nlp.uned.es/weps/}} and the RepLab 2012 tackle unknown-entity scenario approaches, while the RepLab 2013 \footnote{\url{http://nlp.uned.es/replab2013/}} focus on the known-entity scenario approach.

In the WePS-3, the LSIR \cite{kn:Yerva2010} research group has build a system where a profile identify each of the companies mention. The Wordnet \footnote{\url{https://wordnet.princeton.edu/}} and the company web-page were used to extract a bunch of keywords related with the company. Combining this previously set of keywords with some manually defined they have created the profiles to the companies in analyse. They used this profiles to extract specific features from "tweets" and added to a set where there already was some generic features. This information was further used to classify the tweets with "related/unrelated" labels.

Regarding the ITC-UT \cite{kn:Yoshida2010} research group, they have, firstly, made a prediction of the company name class according to the related-tweet ratio. After this step, a distinct heuristic was found to each of the classes, using basically part-of-speech tagging and the named entity label of the company. Their approach was a two-step classification task.

SINAN \cite{kn:Cumbreras2010} system used an approach of ruled based heuristics, specially the existence of the entity name both on tweets and external resources, such as Wikipedia, DBPedia \footnote{\url{http://wiki.dbpedia.org/}} and the company web-page.

The RepLab 2012 follows an identically problem as the WePS-3, the unknown-entity scenario. Some research teams follow the same approach as S. Yerva et al. \cite{kn:Yerva2010} where the use of profiles describing each company mention to correctly filter the content. DAEDALUS \cite{kn:villena2012} and OXYME \cite{kn:kaptein2012} tackle a manually exploration, such as the development of dictionaries and rules sets to the detection and the classification task, and the selection of feedback terms about the entities, respectively. The automatically methods were explored mainly with external resources. CIRGDISCO \cite{kn:Qureshi2015} and ILPS \cite{kn:peetz2012} used the Wikipedia, while BMEDIA \cite{kn:Chenlo2012} combined it with Freebase to extract related and unrelated concepts. CIRGDISCO proposed a two-step algorithm to solve the filtering task. The first step involves the extraction of the entity related-terms from the Wikipedia and further calculus of the IDF (Inverse Document Frequency) score for each term founded. The second step focus on the idea of concept term score propagation, i.e. to propagate the labels of the high-precision classified tweets to the remaining, in order to increase the recall measure. ILPS tackle the filtering task by using semanticising, where two probabilities are verified: Link Probability and Commonness. The first one represents the probability that an n-gram is linked to an Wikipedia page, while the second is the probability of an n-gram is linked to a certain concept. The ILPS group also used list aggregation and disambiguation techniques to carry out this task.

At RepLab 2013, the filtering task was in a known-entity scenario where the data provided to the groups consists of a collection of tweets about 61 entity names in two distinct languages, English and Spanish. Saleiro et al. \cite{kn:Saleiro2013} have devolved POPSTAR which was the system that, using a supervised learning, has obtained the best results classifying the tweets as related or non-related with the entities. Their group has explored internal features (RepLab Metadata, probabilities in the text, keyword similarities) and external features, such as Web Similarity (between tweet text and the Wikipedia page text) and Freebase scores relatively to the position of the target entity in the retrieved list.

The second best score in the filtering task at RepLab 2013 was obtained by V. Hangya et al. \cite{kn:hangya2013} where their system made usage of text normalization methods, combining the textual features with topic distribution features retrieved by a LDA (Latent Dirichlet Allocation) model. The resulting features were further used in a maximum entropy classifier to perform the filtering task. The LIA \cite{kn:Cossu2013} group has used k-Nearest-Neighbour (kNN) algorithm with a set of discriminant features based on similarity measures. They have used Bag-of-Words representation, combining TF-IDF (Term Frequency-Inverse Document Frequency) with Gini purity criteria, for the tweets collection and calculated the Jacard similarity measure.

This kind of methodologies will be a huge step to validate our dataset since it's important to have only related-topic tweets to analyze the people's feelings, opinions about a correct entity instead unrelated ones.

\subsection{Topic Modeling} \label{subsec:topicmodeling}
The emergence of topic modeling techniques was due to the people's chase of a better understanding of the available information in document corpora. Topic models provide the discovery of certain patterns in a collection of texts and enhance specific words/terms that have a direct relation to the content information \cite{kn:Mehrotra2013}. There are many studies that were conducted in order to prove that is possible to extract coherent topics from micro-blogging data using the LDA (Latent Dirichlet Allocation) model \cite{kn:Mehrotra2013, kn:Hong2010, kn:Zhao2011}. LDA models are difficult to apply to micro-blogging texts because of the characteristics present in this kind of text: short, mixture of contextual clues (URLs, tags, name mentions with the '@'), informal language with many misspelling, acronyms and abbreviations \cite{kn:Mehrotra2013}. L. Hong et al. \cite{kn:Hong2010} describe Latent Dirichlet Allocation as "an unsupervised machine learning technique which identifies latent topic information in large document collections". This technique uses "bag-of-words" to each document which  are represented by a probability distribution over some topics, and each topic is, in turn, represented by a probability distribution over a number of words.

R. Mehrotra et al. \cite{kn:Mehrotra2013} have explored the improvement of the standard LDA model using several pooling schemes of tweets, i.e. aggregating tweets by some characteristics present in its content. Their polling schemes characterization range from basic scheme: where each tweet is treated as a single document; author-wise pooling: aggregating the tweets according its author; burst-score wise pooling: tweets are aggregated by the scores obtained from the execution of a burst detection algorithm; temporal pooling: pools are formed by tweets posted at the same hour; \textit{hashtag}-based polling: the tweets are grouped according to its \textit{hashtag} (\#) reference, and if there are more than one reference then the tweet is added to each of the groups. The authors evaluate the resulting clusters through some metrics, such as the \textit{purity}: verifying the average of the corrected labeled tweets inside the clustering; \textit{normalized mutual information} (NMI): its the calculus of the matching results between the clustering and the category labels; and finally the \textit{pointwise mutual information} (PMI): measure of the statistical independence between two words regarding the close proximity. Their approaches also were studied combining similarity tag assignment (TF and TF-IDF) and the best presented results were performed by \textit{hashtag}-based polling with TF-similarity tag assignment regarding the purity and the NMI metrics, while the best PMI metric was obtained by the simple \textit{hashtag}-based polling method.

L. Hong et al. \cite{kn:Hong2010} also explored the LDA models through a set of schemes formed by them. Their schemes diverge between user-based and term-based groups, where the user-based are agglomerations of messages from the same user while the term-based groups are formed by messages that have the same term in the content. In their approach they have also used Author-Topic Model which is an extension of the LDA model but the main difference is that in the LDA, each document is associated with a multinomial distribution over T topics while in the AT model the association is made to the author instead the document. They used JS Divergence to study the similarity between the performed schemes. The main goal of their work was not the topic modeling but they proved that this sub-task can improve performances of classification, namely when the messages are group by the same user.

W. Zhao et al. \cite{kn:Zhao2011} proposed another extension of the LDA model and named it Twitter-LDA \footnote{\url{https://github.com/minghui/Twitter-LDA}}. Their model follows the idea that each tweet is about some topic, so instead of grouping the tweets into schemes and than extract some topic, they tackle each tweet as a singular problem and extract the target of the content. In their work, the evaluation of the model was made by comparing its effectiveness against the standard LDA model and the Author-topic model. The Twitter-LDA results, obtained from a small set of topics in a preliminary test, have surpass the performance of the others models (standard LDA and Author-topic models).

The last model should be a good start to face the problem of topic modeling in our work, since it's open-source tool and it's available in GitHub.

\section{Sentiment Analysis}
Sentiment Analysis is a task of NLP (Natural Language Processing) and aims the finding of the polarity in opinions, sentiments of people about a specific topic contained in a document or even the overall sentiment present in it. Research done in this area has grown at an impressive pace and this is due to the value that this type of analysis can provide to the business world. "Marketing managers, PR firms, campaign managers, politicians, and even equity investors and online shoppers are the direct beneficiaries of sentiment analysis technology" since the retrieved information can favor and make easier the decision-making process \cite{kn:Feldman:2013}. This task is composed by several distinct problems and there are two main approaches to tackle it: supervised \cite{kn:Saleiro2016, kn:Kouloumpis2011} and unsupervised \cite{kn:Musto2015, kn:Allisio2013}. Feldman in their work \cite{kn:Feldman:2013} enhaces the several types of problems found in the sentiment analysis task. One of them, the document-level sentiment analysis focus on the determination of the sentiment polarity of opinions expressed by the author in his document. Another problem that is widely explored is the sentence-level sentiment analysis which is a deeper version of the previous. A document may have multiple opinions about a specific entity and in order to extract the polarity value about it, a phrase-level split is required. Some countermeasures must be taken into account in the polarization of phrases since the sarcasm component can be present in the content and it's very difficult to treat correctly this. There is another problem in this field named aspect-based sentiment analysis where the sentiment polarity should be directed to the aspects/topics contained in the document. In the following subsections a deep description and studied solution of this problem will be presented.

\subsection{Lexicon-based vs Machine-learning based} \label{subsec:lexi_mach}
Sentiment analysis in Twitter can be divided in three different approaches relatively to the sentiment classification: lexicon-based, machine-learning based or even a hybrid approach between the previous two. In the first place it's necessary to talk about what features are relevant or not in order to tackle this problem. Aggarwal et al. \cite{kn:Aggarwal2012} in his work refer some of the common features used in this problem:

\begin{itemize}
\item \textbf{\textit{Term presence and frequency:}} groups of words, named \textit{n-grams}, and the frequency they occur in the document;
\item \textbf{\textit{POS Tag}}: the existence of adjectives can be relevant indicators to determine the opinion polarization;
\item \textbf{\textit{Opinion words and phrases:}} words that usually transmits some polarity, such as \textit{good and bad}, or even whole phrases that don't have this type of words, e.g. "cost me an arm and a leg";
\item \textbf{\textit{Negation}}: the existence of negative words that may change the opinion orientation, such as "I don't like apples" which means the same as \textit{hate}. 
\end{itemize}

After the features engineering process, it may be necessary to select only a few ones to apply in the classification task. W. Medhat et al. \cite{kn:Medhat2014} mentioned in their work some of the most used methods in this particular step:

\begin{itemize}
\item \textbf{\textit{Lexicon-based:}} It's necessary human annotation. Starts with a small set of seed words and then a bootstrapping methodology is applied to expand the lexicon domain through the discovery of synonyms in external resources;
\item \textbf{\textit{Point-wise Mutual Information}}: It's a statistical method where the co-occurrence level between a given word $w$ and a class $c$ is computed in order to see if a feature is or not correlated with the class. The formula of calculus is given in the equation \ref{eq:pmi},

\begin{equation} \label{eq:pmi}
 M_c(w) = \log(\frac{F(w).p_c(w)}{F(w).P_c}) = \log(\frac{p_c(w)}{P_c})
\end{equation}

where $F(w).P_c$ is the expected co-occurrence level and $F(w).p_c(w)$ is the true value of the co-occurrence.

\item \textbf{\textit{Chi-square}}: It's another statistical method used to measure the correlation between the features and the classes \ref{eq:chi_square},

\begin{equation} \label{eq:chi_square}
X_c^2 = \frac{n.F(w)^2.(p_c(w)-P_c)^2}{F(w).(1-F(w)).P_c.(1-P_c)}
\end{equation}

where $n$ is the total number of documents that composed the collection, $p_c(w)$ represents the conditional probability of class $c$ in the documents containing the word $w$, $P_c$ are the fraction of documents that contain the class $c$ and $F(w)$ are the documents fraction that contain the word $w$.

\item \textbf{\textit{Latent Semantic Indexing}}: It's an unsupervised method that aims the reduction of the original set of features into a new ones through transformation techniques like PCA (Principal Component Analysis)
\end{itemize}

After the conclusion of this step of features selection, the sentiment classification is conducted and there are a very high number of techniques that can be applied.

A. Giachanou et al. \cite{kn:Giachanou2016} divided the \textbf{machine-learning approaches} into three different categories: supervised learning, the classifiers ensembles and deep learning.

Supervised learning methods focuses on the training of classification models with a manually labeled dataset, also named training dataset,  and various features extracted from the Twitter messages in order to submit a test dataset under the model and have an automatic prediction regarding the polarity of the sentiment (positive, negative or neutral) in the message. There are many types of classifiers, such as Naïve Bayes (NB), Maximum Entropy (ME), Support Vector Machines (SVM), Logistic Regression (LR), Random Forest (RF) or even Conditional Random Fields (CRF). In the last few years, many studies of Twitter sentiment analysis using supervised learning were conducted. At 2009, Go et al. \cite{kn:Go2009} tackle a problem of binary classification about Twitter sentiment analysis using SVM, NB and ME algorithms and distant supervision to produce their classifier models. Their dataset was composed by 1.6 million twitter messages and they don't have the problem of imbalanced classes. As features they used POS tags, unigrams and bigrams and they also have in consideration the existence of negation in the messages. The best performance they have obtained was with the Naïve Bayes model and as final conclusions they said that using POS tags as feature doesn't improve the final accuracy. 
Hamdan et al. \cite{kn:hamdan2013} made some experiments using SVM and NB models in Twitter messages but their set of features was different from the previous work mentioned. The authors use DBPedia to extract concepts, WordNet to extract adjectives, Senti-WordNet to extract the sentiment score of some words and also have in consideration the existence of emoticons in the message. As final results they concluded that the SVM model performance surpasses the NB model between 2\%-4\%, and for that they have used the harmonic mean F-measure as evaluation metric. 
Saleiro et al. \cite{kn:Saleiro2016} work with Twitter messages to study the political opinions about the five political leaders during the Portuguese bailout between 2011 and 2014. The features they have used were composed by sentiment aggregate functions applying them to a non-linear regression model using the Random Forests algorithm and to a linear regression model using the Ordinary Least Squares (OLS) algorithm. The authors have grouped the dataset per month in order to see what was the monthly variation. In the validation process, a 10-fold cross validation was used and regarding to the evaluation metric they explore the Mean Absolute Error (MAE).

The Classifiers Ensembles approach is based on the combination of several classifiers to improve the performance of the classification task. The workflow of this approach can be verified in the figure \ref{fig:ensemble-classifiers}. da Silva et al. \cite{kn:daSilva2014} have explored the sentiment analysis in Twitter using a combination between Random Forests, Support Vector Machines, Multinomial Naïve Bayes and Linear Regression. The final classification decision in this kind of approach is usually made by majority voting between the models. In their work, da Silva et al. decided to calculate the average between all classification probabilities given by the models and applied that resulting value to the decision task.

\begin{figure}
  \caption{The workflow of the classifiers ensembles by da Silva et al. \cite{kn:daSilva2014}}
  \centering
    \includegraphics[width=1\textwidth]{ensemble-classifiers}
    \label{fig:ensemble-classifiers}
\end{figure}

Hassan et al. \cite{kn:Hassan2013} have explored a different approach regarding the classifiers ensemble. They proposed a framework that was composed by seven different classification algorithms and used bootstrapping to the sample input in order to provide a small portion to each model. Their set of features had several types: semantic, POS tags, sentiment scores (SentiWordNet) and n-grams. Information Gain criteria was used to the feature selection process. By using bootstrapping in their framework, the problem of imbalance classes was reduced which could be a great advantage to explore in our approach.

Deep learning (DL) is the third and last method in the supervised learning approaches. DL is a recent field in the area of Machine Learning which may imply a scarcity in its addressed use to the sentiment analysis on Twitter \cite{kn:Giachanou2016}.
The studies conducted in this method took advantages from the SemEval-2013 and SemEval-2014 datasets. Tang et al. \cite{kn:tang2014learning} developed three different neural networks models to learn sentiment specific word embeddings (SSWE). The results provide by the models were later used as features to classify the polarity of sentiment in the dataset messages. The authors have combined the obtained features with others like sentiment lexicons \cite{kn:tang2014learning}, emoticons, negation, n-grams, punctuation, clusters, etc \cite{kn:tang2014coooolll}. The evaluation metric used to measure the performance of their classification models was the F-measure. The best result obtained to the SemEval-2013 dataset was 86.58\% while for the SemEval-2014 dataset was 87.61\%.

There are a high diversity in the methods to apply supervised learning, i.e. application of machine-learning algorithms, to automatic classify the sentiment polarity either on tweets or opinion reviews. The problem in its use focuses on the features engineering which is a hard task and the learning algorithms effectiveness depends on its selection. The bad choice of some features may cause that the final results obtained are not the most desirables.

Contrary to the machine-learning approaches, the \textbf{lexicon-based approaches} doesn't depend on training data and features to classify the sentiment as positive, negative or neutral. In this approach the final sentiment classification is given by measuring the sentiment score of each term using external resources, such as dictionaries with a large number of previously evaluated terms (SentiWordNet \footnote{\url{http://sentiwordnet.isti.cnr.it/}}, SenticNet \footnote{\url{http://sentic.net/}}, LIWC \footnote{\url{http://liwc.wpengine.com/}}). At 2010, Thelwall et al. \cite{kn:Thelwall:2010} developed a lexicon-based algorithm, named SentiStrength \footnote{\url{http://sentistrength.wlv.ac.uk/}}, capable of detecting the sentiment value in messages that usually have informal language, such as tweets. The algorithm have access to 298 positive terms and 465 negatives and to a list of emoticons, negations and booster words to increase or decrease the sentiment value of derived words. The authors have compared their algorithm with machine-learning approaches and the results were very interesting since in terms of accuracy as evaluation metric, SentiStrength has surpass the others.

C. Musto et al. \cite{kn:Musto2015} have developed a domain-agnostic framework to produce some social media analytics regarding some events that happen in Italy in the last years. They evaluate the sentiment present in each tweet using lexicon-based approaches. The external resources used were SenticNet and SentiWordNet. The authors have split each tweet message by cues, such as punctuations and conjunctions, creating two or more micro-phrases. After this step, each micro-phrase is classified according the scores of the terms present in the resources. The sentiment polarity of the original message is obtained by summing its related micro-phrases. They also studied an emphasized approach where the Part-of-speech (POS) category of each term has a weight. Adverbs, verbs and adjectives received a value greater than 1, while for the remaining categories the value was 1.

L. Allisio et al. \cite{kn:Allisio2013} proposed a framework, named Felicittà, in order to measure the hapiness level in the Italian territory. The study was made on geotagged tweets and it was used the resources MultiWordNet \footnote{\url{http://multiwordnet.fbk.eu/english/home.php}} and WordNet-Affect \footnote{\url{http://wndomains.fbk.eu/wnaffect.html}}. All the emoticons presented in the tweets were replaced by its meaningful words. The approach used by the authors consists in for each tweet term, a search is computed in the MultiwordNet dictionary to find all the meanings the word can have. After this step, each meaning found is associated with the sentiment score present in the WordNet-Affect corpus. The sum of all meanings is calculated, assigning a value of -1, 0 or 1 to the term. The tweet final classification is done by calculating the mean polarity of all terms and comparing with a heuristic constant defined by the authors.

The lexicon-based approaches are simpler to implement compared with the machine-learning approaches. They also presented disadvantages as the need of a continuously update of the word lists (lexicon sentiment dictionaries) because the conversation themes on Twitter are always changing which may result in the absence of words in the lists, and consequently their scores \cite{kn:Giachanou2016}. For this reason, the missing words are not considerate to the sentiment polarity classification and the results may not be so reliables.

The last approach for sentiment analysis is a mixture of the two previously presented, a \textbf{hybrid approach}. This kind of approach was explored by Ghiassi et al. \cite{kn:Ghiassi2013} where they used machine-learning algorithms (SVM and Dynamic Artificial Neural Networks - DAN2) with a n-gram analysis. The collection of tweets was about Justin Bieber and as features to the classifiers, the authors choose emoticons, tweets that have positive and negative words, e.g. \textit{happy or sad}, and also synonyms of this words. The model DAN2 proved be the best in the classification task.
A. Kumar et al. \cite{kn:kumar2012} mixed a log-linear regression model with lexicon-based methods. Firstly, they have made pre-processing to the tweets collection by removing the URL references, replacing emoticons with their score value, calculate the percentage of caps in the message and also the sentiment orientation of the adjectives, verbs and adverbs. The overall sentiment of the tweet message was computed by the linear equation of the model, which was enough to prove the efficiency of the approach explored by the author relatively to the polarity of a tweet.

The main advantage of the hybrid approach establishes in the no need to manually classify the dataset for its use in machine learning methods. By applying lexicon-based methods we can have a labeled dataset ready to be use in ML classifiers. A disadvantage on this approach is high computational power need to bear out both approaches at the same time \cite{kn:Giachanou2016}.

\subsection{Aspect-based Sentiment Analysis}
Aspect-based sentiment analysis (ABSA) is the most difficult problem to solve regarding the field of sentiment analysis. This approach focuses on the recognition of aspects in the messages and consequently on their sentiment polarity classification.

In particular to the aspect extraction, an overview was already done in the subsection \ref{subsec:topicmodeling} where the topic-based approach was mentioned and described using the LDA model as the most used model. There are three more approaches that we can follow to discover relevant aspects in a document: frequency-based \cite{kn:hu2004}, ruled-based \cite{kn:Gindl2013} and supervised learning \cite{kn:Jakob, kn:Jin2009}.

The frequency-based approach focus on finding some nouns or noun phrases from a large corpus using the occurrence frequency as the main requirement. M. Hu et al. \cite{kn:hu2004} used this approach in order to summarize some customer reviews regarding a set of products. They used POS tagging to found nouns and noun phrases present in the document and association mining to find frequent itemsets because the features that composed the itemset are usually product features. After this step, they submitted the features set to a pruning section in order to remove the meaningless ones.

In the ruled-based approach, S. Gindl et al. \cite{kn:Gindl2013} have made a study in order to prove that it's possible identify and extract aspects in sentences by propagating the sentiment charge to noun targets following a set of defined rules. They have verified a problem when the sentiment and the aspect mention are in different sentences. Simple propagation rules are not enough to found the aspects. To overcome this, they have defined another rule where if a sentence starts with a pronoun, the target aspect is probably the last noun identify in the previous sentence.

Supervised learning approaches are based in sequential learning methods, such as Conditional Random Fields (CRF) and Hidden Markov Models (HMM). The mentioned methods are similar because both of them attempt to discover patterns relative to an input data set. These types of methods are often used in the aspect extraction task of opinion mining. N. Jakob et al. \cite{kn:Jakob} has built a classification model using the CRF algorithm in order to enhance the target of each opinion in the reviews. The domain of their dataset was constituted by four independently categories: movies, web-services, cars and cameras. Since the approach taken by the authors was through machine learning classifiers, they had the need of establishing a set of features to train the model. The features used for the classification vary from the string of each token, the POS tag for each token, the level of dependency between each token and the opinion expressed as well as its word distance, and, finally, the last feature is the opinion sentence itself to allow the CRF algorithm the ability of distinguish when a token is present or not in a sentence that is an opinion. Regarding the system validation, the authors applied also a 10-fold cross-validation to see if the model performance improves or not. As performance metrics to evaluate the system, they used the precision (Equation \ref{eq:precision}),

\begin{equation} \label{eq:precision}
\frac{TP}{TP+FP}
\end{equation}

the recall (Equation \ref{eq:recall})

\begin{equation} \label{eq:recall}
\frac{TP}{TP+FN}
\end{equation}

and the F-measure (Equation \ref{eq:f-measure}) that is the harmonic mean between the previous two metrics.

\begin{equation} \label{eq:f-measure}
2.\frac{precision.recall}{precision+recall}
\end{equation}

W. Jin et al. \cite{kn:Jin2009} also worked under the opinion reviews and tried to find relevant opinion targets in the content. They have developed a novel framework based on machine learning techniques. The framework, named OpinionMiner, appeals to a classification model with the HHM algorithm and to a bootstrapping approach in the training step. The bootstrapping divides the main process of training in two sub-process as well as the training dataset into little portions in a randomly way. Each sub-process has his own HHM model and after its training step, the main process only selects the objects which their label is agreed by both classifiers. The bootstrapping process is repeated until no more targets in the objects can be discovered.

Regarding the polarity sentiment classification, the majority of the works studied appeals to one of the approaches described in the section \ref{subsec:lexi_mach}.

\section{Conclusion}

This chapter had the objective of review some basic concepts that may be relevant to contextualize the reader about the problem of performing analysis in social media streams, e.g. Twitter messages. Hence, the literature studied was divided in several points in order to have a overview about what is already done and what are the approaches that some author proposed to tackle each of the sub-problems that composed the main problem in this dissertation work.
After a careful research, it was possible to identify that there are a great diversity of approaches to each sub-problem, whether it be disambiguation, filtering, topic detection or even sentiment analysis.

An important point identified in the literature was the few works done using deep learning to take the problem of sentiment analysis with supervised leaning approaches, since its applicability in the artificial intelligent field has grown at an exponential level in the recent years.

Regardless the task that the authors dealt with, it was possible to identify that the features engineering process and its selection, when their proposed solution used classifier models, is similar. This may be an advantage to the development of the different modules that composed the proposed framework in this dissertation work.

The framework modules will also have classifier models, so the evaluation and validation of it is important. The literature review shows a large set of evaluation metrics to do this step.

In short, it is expected from the reader that this review to the State-of-the-Art has provided a coherent understanding regarding the study of different real scenarios using the social media streams as source of information.
