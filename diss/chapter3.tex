\chapter{Framework} \label{chap:framework}

\section*{}

\section{Requirements}\label{sec:requirements}

\section{Architecture Overview}\label{sec:architecture}

\section{Data Collection}\label{sec:data_collection}

\section{Data Pre-processing}\label{sec:data_preprocessing}

\section{Text Analytics}\label{sec:text_analytics}

\subsection{Word Embeddings}
Mikolov~et~al.~\cite{mikolov2013efficient} has developed a powerful computational method named \emph{word2vec}. The method is capable of learning distributed representations of words, each word being represented by a distribution of weights across a fixed number of dimensions. Authors have also proved~\cite{mikolov2013linguistic} that this kind of representation is robust when encoding syntactic and semantic similarities in the embedding space.

The training objective of the skip-gram model, as defined by Mikolov~et~al.~\cite{mikolov2013linguistic}, is to learn the target word representation, maximizing the prediction of its surrounding words given a predefined context window. For instance, to the word $w_t$, present in a vocabulary, the objective is to maximize the average log probability:

\begin{equation}
\frac{1}{T}  \sum_{t=1}^{T}  \sum_{-c \leq j \leq  c, j \neq 0} \textnormal{log } P(w_{t+j} | w_t)
\end{equation}

where $c$ is the size of the context window, $T$ is the total number of words in the vocabulary and $w_{t+j}$ is a word in the context window of $w_t$. After training, a low dimensionality embedding matrix $\textbf{E}$ encapsulates information about each word in the vocabulary and its use (i.e. the surrounding contexts).

Later on, Q. Le and Mikolov~\cite{le2014distributed} developed paragraph2vec, an unsupervised learning algorithm operating on pieces of text not necessarily of the same length. The model is similar to word2vec but learns distributed representations of sentences, paragraphs or even whole documents instead of words. We used \emph{paragraph2vec} to learn the vector representations of each tweet and tried several configurations in the model hyperparameters.

\subsection{Latent Dirichlet Allocation (LDA)}
D. Blei et al.~\cite{blei2003latent} have developed a generative statistical model that makes possible the discover of unknown groups and its similarities over any dataset. The model tries to identify what topics are present in a document by observing all the words that composing it, producing as final result a topic distribution. An interesting point in this model is that the only features it analyses are the words passing through the training process. The model takes into consideration two different distributions - distribution of words over topics and distribution of topics over the documents - being each document seen as a random mixture of topics and each topic as a distribution of words.

\section{Visualization}\label{sec:visualization}

