\chapter{Framework} \label{chap:framework}

\minitoc \mtcskip \noindent

In this chapter it is described the details and specificities of the framework proposed in this dissertation. First, we enunciate the necessary requirements to fulfill and achieve the mentioned development. Moreover, it is present the design of the framework, as well as its inner structure. The modules that constitutes such architecture are described afterwards as so the required methodologies and algorithms incorporated in each of its tasks. Finally but not least, we enunciate the different data visualizations available on the framework.

\section{Requirements}\label{sec:requirements}

The development of frameworks to the domain of \textit{smart cities} and intelligent transportation systems using human-generated content (e.g. text messages) is a laborious and time-consuming process. The source of the data to fed such system is one of the biggest challenges in this kind of developments, ranging from social media, smart phones and urban sensors. In this dissertation we tackle the problem of exploring social media data since this kind of data have, recently, been seen as a new opportunity and source to mine valuable information to the cities services and corresponding responsible entities~\cite{kn:Musto2015}.

Social media data are mostly represented by text messages being necessary the application of Natural Language Processing (NLP) methodologies in order to extract information from its content. Such methodologies are usually complex and composed by several different steps (e.g. some related to the syntax of the sentences while others are related to the semantics of its content) before the achievement of the desired results. Social Media streams are no exception, indeed, the analysis of such texts is even more complex since messages are usually short and present lots of informal characteristics.

A framework for the domain of social media content requires, in the first place, a data collection module. Depending on the social network, the data collection module can have different heuristics with respect to the data retrieving. Here, the choice of such heuristics is important and needs to be made according the final users expectations, or at least, according the framework final use case. Towards the application of NLP techniques, a module in charge of preprocessing tasks is required. The main purpose of this module establishes in the performance and robustness of the results obtained by the previously mentioned techniques. NLP techniques can provide different types of information, however in this dissertation the focus is on the classification of travel-related tweets, characterization of the topic associated with a tweet and also travel-mode extraction. Each technique is represented as an independent module whose belongs to the boundary of text analytics. The framework needs to be capable of processing information regarding the creation date of a tweet, the \textit{metadata} and the geographic distribution associated to it.

\section{Architecture Overview}\label{sec:architecture}

\section{Data Collection}\label{sec:data_collection}

%In the particular case of Twitter, there are three diffewhich enables different type of dat

%mac2016effects

\section{Data Pre-processing}\label{sec:data_preprocessing}

\section{Text Analytics}\label{sec:text_analytics}

\subsection{Word Embeddings}
Mikolov~et~al.~\cite{mikolov2013efficient} has developed a powerful computational method named \emph{word2vec}. The method is capable of learning distributed representations of words, each word being represented by a distribution of weights across a fixed number of dimensions. Authors have also proved~\cite{mikolov2013linguistic} that this kind of representation is robust when encoding syntactic and semantic similarities in the embedding space.

The training objective of the skip-gram model, as defined by Mikolov~et~al.~\cite{mikolov2013linguistic}, is to learn the target word representation, maximizing the prediction of its surrounding words given a predefined context window. For instance, to the word $w_t$, present in a vocabulary, the objective is to maximize the average log probability:

\begin{equation}
\frac{1}{T}  \sum_{t=1}^{T}  \sum_{-c \leq j \leq  c, j \neq 0} \textnormal{log } P(w_{t+j} | w_t)
\end{equation}

where $c$ is the size of the context window, $T$ is the total number of words in the vocabulary and $w_{t+j}$ is a word in the context window of $w_t$. After training, a low dimensionality embedding matrix $\textbf{E}$ encapsulates information about each word in the vocabulary and its use (i.e. the surrounding contexts).

Later on, Q. Le and Mikolov~\cite{le2014distributed} developed paragraph2vec, an unsupervised learning algorithm operating on pieces of text not necessarily of the same length. The model is similar to word2vec but learns distributed representations of sentences, paragraphs or even whole documents instead of words. We used \emph{paragraph2vec} to learn the vector representations of each tweet and tried several configurations in the model hyperparameters.

\subsection{Latent Dirichlet Allocation (LDA)}
D. Blei et al.~\cite{blei2003latent} have developed a generative statistical model that makes possible the discover of unknown groups and its similarities over any dataset. The model tries to identify what topics are present in a document by observing all the words that composing it, producing as final result a topic distribution. An interesting point in this model is that the only features it analyses are the words passing through the training process. The model takes into consideration two different distributions - distribution of words over topics and distribution of topics over the documents - being each document seen as a random mixture of topics and each topic as a distribution of words.

\section{Visualization}\label{sec:visualization}

