\chapter{Text Analytics Experiments}
\label{chap:experiments}

\minitoc \mtcskip \noindent

In this section we present two different text analytics experiments regarding topic modelling and travel-related tweets classification. First, we manually labelled topics to characterize two Brazilian cities, namely Rio de Janeiro and São Paulo. Later on, we built two travel-related classification models to discriminate tweets for two different speaking languages. For both experiments related to travel classification we manually annotated a English-speaking and a Portuguese-speaking training and test datasets. The remainder sections described each experiment as well as discussion and analysis of the obtained results.

%Providing to the final users of our framework actionable and trustful information about the smart cities and transportation domains is a laborious and time consuming process. First, in order to extract information it is necessary the conduction of some experiments to test the text analytics modules described in Section~\ref{sec:text_analytics}. Second, the results obtained from each experiment need to be evaluated in order to prove robustness and efficiency in our implemented modules. Last but not least, it is necessary to enhance the value and importance of such analysis and what indeed social media content can provide to improve services and the whole cities that use our framework.

%Hence, in this Chapter, we describe three different experiments that were perform during this dissertation period to implement the previous mentioned text analytics modules

\section{Topic Modelling}\label{sec:topic_modeling}
In this section we describe the experiment of automatic characterization of tweets in two different Brazilian cities, Rio de Janeiro and São Paulo. Here, as previous mentioned in Section~\ref{sec:topic_modelling_framework}, we use \gls{LDA} model to find out which are the latent topics in both cities.
We conduct this experiment using data collected from a period of two months, between March 12 and May 12, 2017. After the data filtering and text pre-processing steps, we obtain a total of 6.6M tweets for Rio de Janeiro and 2.7M tweets for São Paulo.

 We tried training LDA model with 5, 10, 20, 25 and 50 latent topics and manually inspected the top most probable words for each topic. Models with 5, 10, 20 and 25 presented a high number number of overlap terms between topics. Therefore, we opted to proceed with the experiment using 50 latent topics. The number of iterations to train the model was set to 20, in line with the work of Lansley et al.~\cite{lan}.
	
\subsection{Results and Analysis}
\label{subsec:lda_results}
\begin{table}[!ht]
	\centering
	\caption{Example of the topics labels}
	\label{tab:topics_classification}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{c|c}
			\hline
			\textbf{\begin{tabular}[c]{@{}c@{}}Words\\ (20 words most frequent words)\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Topic\\ Labels\end{tabular}} \\ \hline
			\begin{tabular}[c]{@{}c@{}}paulo, vai, hoje, dia, jogo, ser, melhor, time, vamo, brazil, \\ todo, santo, brasil, gol, cara, aqui, agora, corinthiam, ano, palmeiro, vem, ...\end{tabular} & \begin{tabular}[c]{@{}c@{}}Sports and\\ Games\end{tabular} \\ \hline
			\begin{tabular}[c]{@{}c@{}}vou, dia, dormir, queria, hoje, ficar, casa, semano, quero, ter, \\ ainda, hora, agora, sono, aula, acordar, acordei, cedo, fazer, prova, ...\end{tabular} & \begin{tabular}[c]{@{}c@{}}Wake-up\\ Messages\end{tabular} \\ \hline
			\begin{tabular}[c]{@{}c@{}}top, social, artist, vote, the, award, army, bom, voting, doi, \\ bogo, oitenta, sipda, today, vinte, prepara, cypher, oito, quatro, man, ...\end{tabular} & \begin{tabular}[c]{@{}c@{}}Voting and\\ Numbers\end{tabular} \\ \hline
			\begin{tabular}[c]{@{}c@{}}marco, nada, falar, emilly, gente, quer, nao, pessoa, nunca, fala, \\ vai, falando, sobre, chama, agora, manda, vem, mensagem, vivian, bbb, ...\end{tabular} & \begin{tabular}[c]{@{}c@{}}Big Brother\\ Brazil 2017\end{tabular} \\ \hline
			\begin{tabular}[c]{@{}c@{}}paulo, brazil, sao, santo, vila, just, parque, posted, photo, shopping, \\ paulista, centro, bernardo, jardim, cidade, avenido, praia, santa, campo, academia\end{tabular} & \begin{tabular}[c]{@{}c@{}}Tourism and\\ Places\end{tabular} \\ \hline
		\end{tabular}
	}
\end{table}

%We classify the 50 latent topics using the most frequent 50 outputted words for each topic. In Table~\ref{tab:topics_classification} we show the top 20 of the 50 words considered in the characterization of topics, such as Sports and Games, Wake-up Messages, Voting and Numbers, Big Brother Brazil 2017 and Tourism and Places. Although the word-based characterization, we perform a manual inspection over 200 tweets for each topic to confirm if such topic characterization was actually correct or not.

%To evaluate the experimental results obtained for each model (where the difference underlies on the variation of the number of topics), a list with the most frequent 50 words for each topic was extracted. In Table~\ref{tab:topics_classification} we can observe a sample (20 top words) selected out of the 50 studied. Nonetheless, the final evaluation took into consideration all the  50 outputted words.


	
%We also selected and manually analyse a random sample (with the size of 200) of tweets for each topic. This sampling was done in order to get better consistency and trustiness about the classification and characterization of the tweets.

The LDA model does not provide a semantic label for each topic, such as "topic x is about Sports". We inspected the most frequent words of each topic and manually assigned a semantic label. Table~\ref{tab:topics_classification} presents the most frequent words for 5 random topics and the corresponding labels manually assigned. For instance, the topic containing frequent words such as "gol", "jogo", "time" is labelled as "Sports and Games". We follow the semantic taxonomy proposed by Lansley~\cite{lansley2016geography} to manually labelling each topic.

We observed that many different latent topics were about the same semantic subject but at different levels of granularity. For instance, "European Football \emph{vs} Brazilian Football". We decided to manually aggregate these overlapping topics to create a simpler and easier to analyse list of topics, resulting in a total of 29 aggregated topics. We performed this process to both cities independently.

\begin{table}[!ht]
	\centering
	\caption{Final results of the LDA topics aggregation. (*) topic labels different from Lansley~\cite{lansley2016geography}}
	\label{tab:topic_labels}
	\resizebox{\textwidth}{!}{
		\begin{tabular}{l|S[table-format=7.0]S[table-format=2.2]|S[table-format=7.0]S[table-format=2.2]|rS[table-format=2.2]}
			\hline
			\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Topic Label}}} & \multicolumn{2}{c|}{\textbf{Rio de Janeiro}} & \multicolumn{2}{c|}{\textbf{S\~ao Paulo}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Diff (\%)}}} \\ \cline{2-5}
			\multicolumn{1}{c|}{} & \textbf{No. Tweets} & \textbf{Percentage (\%)} & \textbf{No. Tweets} & \textbf{Percentage (\%)} & \multicolumn{1}{c}{} \\ \hline
			Academic Activities (*) & 101,590 & 1.54 & 90,616 & 3.30 & -1.76 \\
			Actions or Intentions & 600,030 & 9.12 & 128,710 & 4.69 & \textbf{+4.43} \\
			Antecipation and Socialising & 132,606 & 2.01 & 0 & 0.00 & \textbf{+2.01} \\
			BBB17 (*) & 122,054 & 1.85 & 68,385 & 2.49 & -0.64 \\
			Body, Appearances and Clothes & 160,342 & 2.44 & 71,447 & 2.60 & -0.17 \\
			Food and Drink & 167,204 & 2.54 & 58,407 & 2.13 & +0.41 \\
			Health & 119,013 & 1.81 & 0 & 0.00 & \textbf{+1.81} \\
			Holidays and Weekends & 104,695 & 1.59 & 79,610 & 2.90 & -1.31 \\
			Informal Conversations & 272,502 & 4.14 & 138,848 & 5.06 & -0.92 \\
			Live Shows, Social Events and Nightlife & 359,342 & 5.46 & 140,240 & 5.11 & +0.35 \\
			Mood & 139,287 & 2.12 & 138,399 & 5.04 & \textbf{-2.92} \\
			Movies and TV & 285,198 & 4.33 & 39,778 & 1.45 & \textbf{+2.89} \\
			Music and Artists & 84,407 & 1.28 & 78,142 & 2.85 & 1.56 \\
			Negativism, Pessimism and Anger(*) & 229,104 & 3.48 & 183,050 & 6.67 & \textbf{-3.18} \\
			Numbers, Quantities and Classification & 86,897 & 1.32 & 78,160 & 2.85 & -1.53 \\
			Optimism and Positivism & 106,714 & 1.62 & 39,725 & 1.45 & +0.18 \\
			Personal Fellings & 375,735 & 5.71 & 532,331 & 19.38 & \textbf{-13.67} \\
			Politics & 81,254 & 1.23 & 46,758 & 1.70 & 0.47 \\
			Relationships and Friendship (*) & 1,524,804 & 23.17 & 187,541 & 6.83 & \textbf{+16.34} \\
			Religion & 183,174 & 2.78 & 66,788 & 2.43 & +0.35 \\
			Routine Activities & 334,216 & 5.08 & 82,421 & 3.00 & +2.08 \\
			Slang and Profinities & 241,676 & 3.67 & 44,620 & 1.62 & +2.05 \\
			Social Media Applications & 105,809 & 1.61 & 44,073 & 1.60 & +0.01 \\
			Sport and Games & 382,479 & 5.81 & 133,047 & 4.84 & +0.97 \\
			Tourism and Places & 59,288 & 0.90 & 86,519 & 3.15 & -2.25 \\
			Transportation and Travel & 130,261 & 1.98 & 63,923 & 2.33 & -0.35 \\
			Weather (*) & 91,302 & 1.39 & 42,588 & 1.55 & -0.16 \\
			Shopping (*) & 0 & 0.00 & 44,470 & 1.62 & \textbf{-1.62} \\
			Voting (*) & 0 & 0.00 & 37,687 & 1.37 & \textbf{-1.37} \\ \hline
			\textbf{Total} & 6,580,983 & 100.00 & 2,746,283 & 100.00 &  \\			\hline
		\end{tabular}
	}
\end{table}

%During the characterization of the 50 different topics, we conclude the existence of topics which theme was the same (e.g. Love and Romance Problems or Brazilian Football \textit{versus} European Football). Within this, such groups were aggregated into the same topic, \textit{Relationships} and \textit{Sports and Games}, respectively. After this grouping process, a total of 29 different topics was achieved.

%It was found a group of 50 topics which had the largest number of distinct topics between them. We choose to use this model trained to characterize tweets into 50 topics because the final goal of our framework is to be able of compare or monitor cities, regions or even countries having into consideration high levels of diversification and generalization. However, by studying the 50 different topics, we conclude the existence of topics which theme was the same (e.g. Love and Romance Problems or Brazilian Football \textit{versus} European Football). Within this, such groups were aggregate into the same topic, \textit{Relationships} and \textit{Sports and Games}, respectively. After this grouping process, a total of 29 different topics was achieved.



%presented complexity to our characterization objective, such as "\textit{queria namorar um mano parecido com o josh}" (Relationship) and "\textit{como eu queria meus amigos aqui agora cmg}" (Friendship), which raised some doubts about which topic this tweets may belong: Relationship, Friendship or even Actions or Intentions. In a perspective of context, the first tweet belongs to the theme \texttt{flirt}, which is directly related to Relationship. The theme on the second tweet is missing the company of friends, i.e. conviviality, which is related to Friendship. The decision of join the two topics was due to the proximity between them which have as content both types of tweets, talking about love/relationship and friendship, and with this in consideration both topics should be aggregated in order to assure the desired consistency in the characterization.

%The final set of topics (50 topics) to be considered was selected accordantly to the most recurring subjects. 
%The final characterization and details about each topic for both cities is presented in Table~\ref{tab:final_classification}. Almost every topic demonstrated a balanced distribution, with exception of \textit{Relationships and Friendship} and \textit{Personal Feelings} for Rio de Janeiro and São Paulo, respectively. The difference appearing among these topics is a consequence of the final grouping process, since there was a considerable number of words being shared between topics. This issue complicated our characterization task, compelling to an high amount of undesired aggregations.


The resulting list of topics and their distribution (number of tweets) for both cities is depicted in Table~\ref{tab:topic_labels}. We first observe that the majority of topics is common in both cities, with exception of 4 topics:  Weather and Shopping are not discussed in Rio de Janeiro; Antecipation \& Socializing and Health are not discussed in São Paulo. We were not able to assign labels using the Lansley~\cite{lansley2016geography} taxonomy to 7 different topics. In such cases, we created our own labels, e.g., "BBB17" is about a highly popular reality TV show in Brazil named Big Brother Brazil. 

There is a wide range of topics covered by both cities, from "Food and Drink", "Politics" and "Religion" to "Sports and Games" or "Transportation and Travel". The most talked topics in Rio de Janeiro are "Relationships and Friendship", "Actions and Intentions" and "Sports and Games", while in São Paulo, the most talked about topics are "Personal Feelings", "Relationships and Friendship" and "Negativism, Pessimism and Anger". Comparing both cities, the topics with higher relative difference are "Relationships and Friendships" (+16\% in Rio de Janeiro) and "Personal Feelings" (+13\% in São Paulo).
%The most talked topics in Rio de Janeiro are about specific situations. For instance, people tend to talk about quotidian activities, where it is possible to be proved by the latent topic "Routine Activites". Moreover, nightlife activities as well as music or theatrical live shows are fields of interest to their inhabitants. "Actions and Intentions" demonstrate high number of tweets meaning that people is always moving, doing something or at least wishing for it.
%On the other hand, São Paulo latent topics are not so lively, 

We also produced a day-of-the-week temporal distribution of topics in both cities as depicted in Figure~\ref{fig:topics_heat_maps}. We selected 12 topics for Rio de Janeiro and 13 for São Paulo that are more prone to temporal shift of popularity, such as "Religion" and "Sports and Games" which are presumably more popular on the weekends. For both cities the topic "Sports and Games" is more mentioned on Tuesdays and Saturdays. Indeed, this observation correlates with Tuesdays where \textit{UEFA Champions League} competition happens and Saturdays when occur \textit{Brazilian Football League} matches. Also in both cities, "Holidays and Weekends" presented Sundays as the day where more people talk about it, while "Religion" and "Tourism and Places" are less prone to be talked about in Fridays, which is similar to all the remaining topics. "Live Shows, Social Events and Nightlife" are more talked about on Saturdays in Rio de Janeiro, while on São Paulo we identify Tuesdays as the day more popular. Sundays and Tuesdays are the days when the well-famous reality show TV program is emitted, and due to that exists more popularity in these days for the "BBB17" topic. "Weather" topic presents more activity in Saturday probably due to people going out to take a walk.

\begin{figure}[!t]
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/rio_topics_heatmap.pdf}
		\caption{Rio de Janeiro}
		\label{fig:rio}
	\end{subfigure}
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth]{figures/sp_topics_heatmap.pdf}
		\caption{São Paulo}
		\label{fig:sp}
	\end{subfigure}
	\caption[Day-of-the-week Twitter activity]{Day-of-the-week activity per each topic in both cities}
	\label{fig:topics_heat_maps}
\end{figure}

%Furthermore, it is worth mentioning that our model had successfully discover a topic related to Big Brother Brazil 2017 (BBB17), a well-known reality show. The amount of geo-located tweets concerning this topic was considerable (1.85\% and 2.49\%, in RJ and SP, respectively), rising the question about what led people to geo-located them in such topic.

\subsection{Final Remarks}
To the best of our knowledge, this is the first large scale analysis of topic modelling of geo-located tweets from Rio de Janeiro and São Paulo. Most of the topics are common to both cities. Is is interesting to notice that people in Brazil post geo-located tweets about general purpose topics, such as reality TV show, health issues and relationship and friendship. Transportation and travel are marginal topics with less 2\% of relative frequency in Rio de Janeiro and 2.3\% in São Paulo.

%LDA models usually require documents of large size, or at least ones with higher complexity than a single tweet so as to yield appropriate performance. A traditional approach was followed considering each tweet as a document instead of trying to aggregate tweets in more complex documents taking into consideration some criteria, e.g. grouping messages by date and hour. All topics resulting from our approach are similar in both cities but two, which are unique for each of the selected scenarios. The percentage difference between similar topics was within the interval 0.16-4.43\% evidencing the fact that both cities are also similar besides different factors that characterize each other: population, culture, lifestyle and also the region where the city is located in.

%In spite of the analysis carried out and reported in this experiment, we can not assure that inside a topic other encapsulated topics might exist. The resulting amount of tweets for each topic was extremely high, turning a one-by-one verification into a very laborious and time consuming process. Therefore, our classification approach was limited to the verification of the top-50 words and the manual identification of such words in samples of 200 tweets per topic. Such a limitation, nonetheless, did not prevent us to draw important conclusions relatively to the results we have obtained after the application of our proposed solutions, as discussed in the Section \ref{sec:results}.

This experiment demonstrates the capability of our framework to handle different topic modelling analysis under unregulated and non-conventional data such as the content found in most social media. The application of topic modeling technique to tweets from two different cities enables interesting comparisons between them since the whole analytics process accounts for what inhabitants talk about in their social networks. Through these analysis, cities' services are capable of monitoring human behaviour, activity patterns as well as of identifying regions where there may be some levels of intolerance on certain topics, making it possible to trigger preventive measures to solve problems in those specific areas.

Future direction for this research will include application of spatio-temporal aggregation methods over both datasets in order to create meta-documents (tweets group by day/hour/location) and verify whether results can be different taking into consideration temporal and spatial factors. To pursue this, it is required that a large dataset for both cities is available, which is expectable only in mid- to long-term.

\section{Travel-related Classification}
\label{sec:travel_related_classification}
The main goal of this section is to describe the experiments conducted to discriminate travel-related tweets in Rio de Janeiro, São Paulo and New York City. Considering the volume of the collected data for each scenario, it is necessary to automatically identify tweets whose content somehow suggests to be related to the transportation domain. Conventional approaches would require us to specify travel-related keywords to classify such tweets. On the contrary, our approach consisted in training a classification model to automatically discriminate travel-related tweets from non-related ones. 

One big challenge always present in text analysis is the sparse nature of data, which is especially the case in Twitter messages. Conventional techniques such as bag-of-words tend to produce sparse representations, which become even worse when data is composed by informal and noisy content.

Word embeddings, on the other hand, is a text representation technique that tries to capture syntactic and semantic relations from words. The result is a more cohesive representation where similar words are represented by similar vectors. For instance, \emph{"taxi"/"uber"}, \emph{"bus/busão/ônibus"}, \emph{"go to work"/"go to school"/"ir para a escola"} would yield similar vectors respectively.
We are particularly interested in exploring the characteristics of word embeddings techniques to understand which extent it is possible to improve the performance of our classifier to capture such travel-related expressions. In the reminder subsections, we describe two different text classification experiments following distinct approaches across two speaking languages - Portuguese and English.

Support Vector Machines (SVM), Logistic Regression (LR) and Random Forests (RF) were the classifiers used in these experiments. The SVM classifier was tested under three different kernels, namely \textit{rbf}, \textit{sigmoid} and \textit{linear}; the latter proved to obtain the best results for both experiments. 

The LR classifier was used with the standard parameters, whereas the RF classifier used 100 trees in the forest. The gini criterion and the maximum number of features were limited to those as aforementioned in Section~\ref{sec:travel_features}, in the case of the RF classifier.

To evaluate the performance of classifiers in our experiences we used five different metrics. Firstly we compute a group of three per-class metrics, namely precision, recall and the F1-score. Bearing in mind this study considers a binary classification, metrics were associated with the travel-related class only, i.e. the positive class.

We established the use of different groups of features to train our classification model, namely bag-of-words, bag-of-embeddings - word embeddings dependent technique - and both combined (horizontally combination of bag-of-words and bag-of-embeddings matrices into a single one).

\subsection{Rio de Janeiro and São Paulo}
\label{subsec:rio_de_janeiro_sao_paulo_experiment}

Messages were collected for a period of a whole month, between days March 12 and April 12, 2017, and the resulting datasets sum up a total of 6.1M and 2.9M tweets for Rio de Janeiro and São Paulo, respectively. Due to the problem detected in Section~\ref{sec:geographical_distribution}, we filtered the data in order to use only tweets that were actually inside the cities' areas. The final composition of the datasets is presented in Table~\ref{tab:brazilian_datasets_travel}, and the subset of data considered in this experiment sum up a total of 7.7M tweets -  5.3M and 2.4M tweets for Rio de Janeiro and São Paulo, respectively.

\begin{table}[ht]
	\small
	\centering
	\caption{Portuguese datasets composition for the travel-related classification experiment}
	\label{tab:brazilian_datasets_travel}
	\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			\textbf{City}  & \textbf{All} & \textbf{PT} & \textbf{Non-PT} & \textbf{\begin{tabular}[c]{@{}c@{}}Inside \\ Bounding-Box\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}Outside\\ Bounding-Box\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}PT and Inside \\ Bounding-Box\end{tabular}} \\ \hline
			Rio de Janeiro & 6,175,000 & 5,355,000 & 819,000 & 4,327,000 & 1,848,000 & 3,749,000 \\ \hline
			São Paulo      & 2,934,000 & 2,444,000 & 490,000 & 2,016,000 & 918,000 & 1,672,000 \\ \hline
		\end{tabular}}
	\end{table}

\subsubsection{Training and Test Datasets}
\label{subsec:training_test_datasets_portuguese}
The construction of the training and test sets followed a semi-automatic labeling approach. We tried to built a balanced training set concerning the travel-related class and the non-related. The selection process of tweets have support on a strategy used in the study of Maghrebi et al.~\cite{maghrebi2016transportation}, which consists in searching tweets from a collection using specific travel terms in conjecture with white-spaces in the start and end of a regular expression (e.g. \emph{" carro "}, for the car mode of transport). Using the terms declared in Table~\ref{tab:terms} combined with the previous mentioned regular expression, we found about 30,000 tweets. From this subset, we randomly selected a small sample of 3,000 tweets to manually confirm if they were indeed related to travel topics. Although the randomly selection of tweets to produce such training set, we careful analyse the existence of all modes of transportation present in Table~\ref{tab:terms}. After this manual annotation we selected 2,000 tweets and used them as positive samples in the training dataset.

\begin{table}[htbp]
	\centering
	\small
	\caption{Travel terms used to build the training set}
	\label{tab:terms}
	\begin{tabular}{c|c|c}
		\hline
		\multirow{2}{*}{\textbf{Mode of Transport}} & \multicolumn{2}{c}{\textbf{Terms}} \\ \cline{2-3} 
		& \multicolumn{1}{l|}{\textbf{Portuguese Language}} & \textbf{English Language} \\ \hline
		\textbf{Bike} & bicicleta, moto & bicycle, bike \\
		\textbf{Bus} & onibus, ônibus & bus \\
		\textbf{Car} & carro & car \\
		\textbf{Taxi} & taxi, táxi & taxi, cab \\
		\textbf{Train} & metro, metrô, trem & metro, train, subway \\
		\textbf{Walk} & caminhar & walk \\ \hline
	\end{tabular}
\end{table}

In order to select negative samples for the training dataset we randomly selected 2,000 tweets and also manually verified their content to assure that they were not travel-related. Finally, our training set was composed by 4,000 tweets, from which 2,000 were travel-related and 2,000 were not. 
We selected 1,000 tweets randomly that were not present in the training set to build the test set, and then manually classified them as travel-related or non-travel-related. In the end, 71 tweets were found to be travel-related and whereas 929 were not.

It is worth mention that we try to hide some terms from the training set in order to verify if the embeddings were able to discriminate tweets about \emph{"Uber"/"Busão"}, which are terms related to the \texttt{car/taxi} and \texttt{bus} modes of transport, respectively. To assure such test, we incorporate specific-related tweets about these modes of transport in the 71 travel-related tweets of the test set. By obtaining good result regarding the performance of the classification model, we can induce several advantages regarding the use of word-embeddings-based features in social media classification tasks for a large diversity of domains, such as the smart cities and transportation domains.

\subsubsection{Results and Analysis}
\label{subsubsec:results_rio_de_janeiro_sao_paulo}
Table~\ref{classifiers} presents the results obtained using the different features combination for our test set composed by 1,000 tweets manually annotated. According to the evaluation metrics we conclude that the bag-of-word and bag-of-embeddings combined produced better classification models. The model produced by the Linear SVM performed slightly better than the LR and the RF. Interesting to note is that \gls{BoW} features have influence on the precision scores obtained from our results, producing more conservative classifiers. Regarding the recall results, we can see that the Logistic Regression using only bag-of-embeddings features was the model with best results; perhaps if the precision is taken into consideration, the same conclusions will not be possible. Analysing the scores provided in Table~\ref{classifiers}, the best model under the F1-score was the Linear SVM, with a score of 0.85. It is worth noting that combining Bag-of-words and Bag-of-embedding with size 100 was the group of features with best performance taking into consideration the evaluation metrics used in this experiment.

\begin{table}[!bp]
	\small
	\centering
	\caption{Performance results with 100 sized vectors for BoE}
	\label{classifiers}
	\begin{tabular}{c|c|c|c|c}
		\hline
		\textbf{Classifier}                  & \textbf{Features} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\ \hline
		\multirow{3}{*}{Linear SVM}          & BoW               & 1.0                & 0.6761          & 0.8067            \\
		& BoE               & 0.4338             & 0.8309          & 0.5700            \\
		& \textbf{BoW + BoE}         & \textbf{1.0}       & \textbf{0.7465} & \textbf{0.8548}   \\ \hline
		\multirow{3}{*}{Logistic Regression} & BoW               & 1.0                & 0.6338          & 0.7759            \\
		& BoE               & 0.4444             & 0.8451          & 0.5825            \\
		& BoW + BoE         & 1.0                & 0.6761          & 0.8067            \\ \hline
		\multirow{3}{*}{Random Forest}       & BoW               & 1.0                & 0.6338          & 0.7759            \\
		& BoE               & 0.2298             & 0.8028          & 0.3574            \\
		& BoW + BoE         & 1.0                & 0.6338          & 0.7759            \\ \hline
	\end{tabular}
\end{table}


\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/roc_auc_brazilian_travel_related}
	\caption{ROC Curve of SVM, LR and RF experiences}
	\label{fig:roc_curve}
\end{figure}

\begin{figure}[!hbp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/predicted_day_of_week}
	\caption{Positive Predicted Tweets per Day of Week}
	\label{fig:predicted}
\end{figure}


The performance of all three classifiers is illustrated using the ROC Curve in Figure~\ref{fig:roc_curve}.

The area under the curve of the Receiver Operating Characteristic (AUROC) was very similar for both the Logistic Regression and the Linear SVM models. The results obtained from the Random Forest model were not so promising as expected.

After the selection of our classification model, we decided to classify all the Portuguese dataset and draw some statistics from the results. The trained Linear SVM classifier was used to predict whether tweets were travel-related or not, since it was the model presenting the best score under the F1-score metric (as shown in Table~\ref{classifiers}). From a total of 7.8M tweets, our classifier was able identified 37,300 travel-related entries.

\begin{figure}[!bp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/rio_1}
	\caption{Rio de Janeiro heat map to the positive tweets}
	\label{subfig:rio_heatmap}
\end{figure}

\begin{figure}[!htp]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/sp_1}
	\caption{São Paulo heat map to the positive tweets}
	\label{subfig:sp_heatmap}
\end{figure}

Figure~\ref{fig:predicted} depicts the distribution of travel-related tweets over the days of the week. We can see that the first three business days (Monday, Tuesday and Wednesday) are the ones on which the Twitter activity is higher for both cities in our study.

In order to understand the spatial distribution of travel-related tweets we generated a heatmap for both cities. From the heatmap of Rio de Janeiro, illustrated in Figure~\ref{subfig:rio_heatmap}, it is possible to identify that some agglomerations of tweets are located at Central do Brasil, Cidade Nova and Triagem train stations, as well as at Uruguaiana, Maracanã and Carioca metro stations. The Rio-Niterói bridge, connecting Rio de Janeiro to Niterói, as well as the piers on both sides also presented considerable clouds of tweets classified as travel-related.

The heatmap for the city of SP, illustrated in Figure~\ref{subfig:sp_heatmap}, was also an interesting case to observe. Almost every agglomeration matched some metro or train station. Estação Brás, Tatuapé, Belém, Estação Paulista, Sé, Liberdade were some of the stations highlighted in the heatmap. We could also identify a little agglomeration of travel-related tweets at Congonhas airport, even though no tweets seemed to mention the word \textit{plane} explicitly in the training of our classification model.

\subsubsection{Final Remarks}
The previous described experiment explores an approach of supervised learning using as training examples a set of manually annotated tweets extracted from the whole datasets with the support of a term-based regular expression. The overall methodology is concerned with the problem of construct a fine-grained Twitter training set for the travel domain and also the automatic identification of travel-related tweets from a large scale corpus. We combined different word representations to verify whether our classification model could learn relations between words at both syntactic and semantic levels. After using standard techniques such as bag-of-words and bag-of-embeddings, we have used them combined yielding results that showed that these different groups of features can complement each other, with respect to Portuguese-speaking tweets. Modes of transport are always evolving and new services emerges making the identification of tweets related to it difficult. Overall, our experiment proved that word-embeddings features are actually an advantage regarding its applicability into instable real-world scenarios such as the transportation domain. 

\subsection{New York City}
\label{subsec:new_york_city_experiment}

Similar to the experiment of Portuguese-speaking travel-related classification of tweets, we built a model to discriminate English-speaking travel-related tweets. However, the construction of the training and test sets in this experiment follows a different approach. While in the Rio de Janeiro and São Paulo experiment we explore an semi-automatic approach and tweets were almost instantaneous formed as a group, here we were obligate to follow a two-phase approach due to the polysemy level of English travel terms.

Differently from the Brazilian cities experiment, tweets were collected from New York City during a period of two months, between days March 12 and May 12, 2017. Ignoring all non-English, as well as tweets located outside the bounding-box of New York City, the resulting dataset comprehends 4M tweets.

Regarding the preparation of data, we used the same preprocessing operations in both experiments, Brazilian and North-American. The operations were lowercasing, transformation of repeated characters and cleaning of \emph{entities} (user mentions and URLs) from the message content.

\subsubsection{Training and Test Datasets}

In the Portuguese dictionary, travel-related terms do not have more than one meaning. For instance \emph{"caminhar"} or even \emph{"comboio"} possesses only one meaning. Regarding the English dictionary, travel-related tweets may have more than one meaning since some of them present high level of polysemy. Terms such as \emph{"walk"} may be used to describe the action of walk or, for example, the action of \emph{walk into}. On the other hand, the term \emph{"train"} can be used to describe the mode of transport train or a type of behaviour through practice and instruction.

The polysemy level of such terms was took into consideration while the construction process of our training set of tweets for the English-language travel-related classification model. In the first stage of the construction process, we used the same strategy of the Portuguese training set. By take support on a semi-automatic labeling technique using a regular expression, we find out almost 16,000 tweets. The next step in the construction process was a manually verification followed by a manually annotation. Overall, 1,686 tweets were selected for each of both binary classes, travel-related and non-related. The travel-related set was strictly balanced in order to have almost the same amount of examples for each of the travel-modes involved in this study. The non-related training set is composed of several subjects that are not related to travel, e.g. football, leisure, politician, personal tweets, among others.

Nonetheless, we include into the training set tweets which polysemy level may induce doubts regarding the context of the message in order to make possible higher levels of discrimination in our model. This inclusion may help the learning process of our model making it capable of correctly identify which are the tweets that are actually related to the travel and transportation domain.
The final composition of the training datasets is presented in Table~\ref{tab:new_york_first_dataset}.

\begin{table}[!tp]
	\centering
	\caption[English-speaking tweets datasets]{Composition of the training and test datasets for the English travel-related tweets classification}
	\label{tab:new_york_first_dataset}
	\begin{tabular}{c|c|c|}
		\cline{2-3}
		\textbf{}                                       & \multicolumn{2}{c|}{\textbf{Training Set}}                          \\ \hline
		\multicolumn{1}{|c|}{\textbf{Mode of Tranport}} & \textbf{Travel-related} & \multicolumn{1}{l|}{\textbf{Non-related}} \\ \hline
		\multicolumn{1}{|c|}{Bike}                      & 300                     & \multirow{6}{*}{1686}                     \\
		\multicolumn{1}{|c|}{Bus}                       & 311                     &                                           \\
		\multicolumn{1}{|c|}{Car}                       & 317                     &                                           \\
		\multicolumn{1}{|c|}{Taxi}                      & 314                     &                                           \\
		\multicolumn{1}{|c|}{Train}                     & 317                     &                                           \\
		\multicolumn{1}{|c|}{Walk}                      & 217                     &                                           \\ \hline
		\multicolumn{1}{|c|}{\textbf{Total}}            & \multicolumn{2}{c|}{3372}                                           \\ \hline
	\end{tabular}
\end{table}

\subsubsection{Preliminary Results}\label{subsec:preliminar_results}
Due to the laborious and time-consuming effort made in the construction of the training set, we opt to apply a different approach in the training phase of our model classification model. In order to enhance the differences between tweets whose terms present high levels of polysemy, the model was trained using a \gls{k-fold-cross-validation} technique with 10 iterations for all groups of features: bag-of-words and bag-of-embeddings and both combined. Results showed good performance for all models regarding the selected evaluation metrics. The best model in this experiment was the Logistic Regression classifier trained with bag-of-words and bag-of-embeddings features, presenting a F1-score of 0,98324.

\begin{table}[!bp]
	\centering
	\caption[New York City First Experiment Results]{Preliminary results (it is only demonstrated the best result for the bag-of-embeddings group)}
	\label{tab:first_experiment}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Classifier} & \textbf{Features} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-score} \\ \hline
		\multirow{3}{*}{\textbf{Linear SVM}} & BoE(200) & 0,90883 & 0,83634 & 0,87089 \\
		& BoW & 0,96298 & 0,97652 & 0,96962 \\
		& \textbf{BoE(200) + BoW} & \textbf{0,97251} & \textbf{0,99114} & \textbf{0,98170} \\ \hline
		\multirow{3}{*}{\textbf{Logistic Regression}} & BoE(100) & 0,90172 & 0,84948 & 0,87447 \\
		& BoW & 0,96431 & 0,98042 & 0,97222 \\
		& \textbf{BoE(200) + BoW} & \textbf{0,97391} & \textbf{0,99285} & \textbf{0,98324} \\ \hline
		\multirow{3}{*}{\textbf{Random Forests}} & BoE(100) & 0,81283 & 0,83600 & 0,82394 \\
		& BoW & 0,96569 & 0,98997 & 0,97764 \\
		& \textbf{BoE(50) + BoW} & \textbf{0,93688} & \textbf{0,99939} & \textbf{0,96701} \\ \hline
	\end{tabular}
\end{table}

The fact that all models performed incredibly well, in particular models using the features group of \gls{BoW} and \gls{BoW}+\gls{BoE} raise to us some questions and doubts about the robustness of the features used in the training process. First, in the Brazilian cities experiment, by following the same approach over the training set construction process we did not obtain results of this kind. Second, the selected tweets are very specific and our model may be overfitted due to training data. In order to pursue and have answers to our questions, we designed another experiment using the same dataset.

\iffalse
\begin{table}[htbp]
	\small
	\centering
	\caption{Preliminary Results}
	\label{tab:first_experiment}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Classifier} & \textbf{Features} & \textbf{Precision} & \textbf{Recall}  & \textbf{F1-score} \\ \hline
		\multirow{2}{*}{\textbf{Linear SVM}} & BoE (200) & 0,90883   & 0,83634 & 0,87089  \\
		& \textbf{BoW} & \textbf{0,96298}   & \textbf{0,97652} & \textbf{0,96962}  \\ \hline
		\multirow{2}{*}{\textbf{Logistic Regression}} & BoE (100) & 0,90172   & 0,84948 & 0,87447  \\
		& \textbf{BoW} & \textbf{0,96431}   & \textbf{0,98042} & \textbf{0,97222}  \\ \hline
		\multirow{2}{*}{\textbf{Random Forests}} & BoE (100) & 0,81283   & 0,83600 & 0,82394  \\
		& \textbf{BoW} & \textbf{0,96569}   & \textbf{0,98997} & \textbf{0,97764}  \\ \hline
	\end{tabular}
\end{table}
\fi

\subsubsection{\emph{Leave-one-group-out}}\label{subsec:leave_one_group_out}

It is worth noting that in our first experiment all travel-mode classes were known by the model before the classification of the test set (the remaining sub-dataset in the 10-fold cross-validation). Comparing with real-world scenarios, this may not be true since new modes of transport and companies, such as Uber, Lyft and Cabify, arise from unpredictable moments. This second experiment follows a \emph{leave-one-group-out} strategy, meaning that one travel-mode class if left out of the training set and moved into the test set. Hence, the behaviour of the learned model when facing a completely unknown travel-mode class can be evaluated. A model for each hidden transport-mode class was built and evaluated using the same training conditions and metrics. The datasets composition of each experiment led in this strategy can be observed in Table~\ref{tab:leave}.

\begin{table}[htbp]
	\small
	\centering
	\caption[\emph{Leave-one-group-out expirement datasets}]{Datasets composition used in the \emph{leave-one-group-out} strategy}
	\label{tab:leave}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Travel-Mode \\ Class\end{tabular}}} & \multicolumn{2}{c|}{\textbf{Training Set}} & \multicolumn{2}{c|}{\textbf{Test Set}} \\ \cline{2-5} & \textbf{Pos.} & \textbf{Neg.} & \textbf{Pos.}  & \textbf{Neg.}  \\ \hline
		Taxi & 1,372 & \multirow{6}{*}{1,686}   & 314 & \multirow{6}{*}{300}  \\
		Train & 1,369 & & 317 & \\
		Car  & 1,369 & & 317 & \\
		Bike & 1,386 & & 300 & \\
		Walk & 1,469 & & 217 & \\
		Bus  & 1,375 & & 311 & \\ \hline
	\end{tabular}
\end{table}

For each experiment of the learning models, we maintain a 10-fold cross-validation approach, however it was built a test set with a hidden travel-mode class and 300 non-related tweets (negative class). Here, only bag-of-words and bag-of-embeddings features were fed into our models classification routine since the main goal of this experiment is to check the features robustness. Table~\ref{tab:results} presents the best results for each model, as so the group of features feeding it. To achieve the final results of this experiment, we calculated the mean between all models' results to each of the hidden transport-mode classes.

\begin{table}[!bp]
	\small
	\centering
	\caption{\emph{Leave one group out} experiments results for SVM, LR and RF classifiers}
	\label{tab:results}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		\textbf{Classifier} & \textbf{Features}  & \textbf{Precision} & \textbf{Recall}  & \textbf{F1-score} \\ \hline
		\multirow{2}{*}{\textbf{Random Forests}} & BoW & 0,40774 & 0,07474 & 0,12629  \\
		& \textbf{BoE (50)}  & \textbf{0,80278} & \textbf{0,76194} & \textbf{0,78447}  \\ \hline
		\multirow{2}{*}{\textbf{Logistic Regression}} & BoW & 0,40774 & 0,07474 & 0,12629  \\
		& \textbf{BoE (50)}  & \textbf{0,84882} & \textbf{0,75702} & \textbf{0,80219}  \\ \hline
		\multirow{2}{*}{\textbf{Linear SVM}} & BoW & 0,41527 & 0,07153 & 0,12203  \\
		& \textbf{BoE (200)} & \textbf{0,86374} & \textbf{0,75715} & \textbf{0,81289}  \\ \hline
	\end{tabular}
\end{table}

According to results, all classification models have performed reasonably well under the bag-of-embeddings features group, although the dimensionality used being different for the Linear SVM classifier.

After testing each model with a hidden travel-mode class, the models trained with bag-of-words features demonstrated poor performance when facing unknown travel-modes, revealing higher sensitivity and lower generalization capabilities in comparison to the bag-of-embeddings version. The generalization power is an important and crucial characteristic for our desired solution since in a real world scenario is very likely that we will face a higher variety of categories that were not taken into consideration in the training phase of our model. Having this considered, the bag-of-words features group presents lack of robustness as we doubt in our first experiment (Section~\ref{subsec:preliminar_results}).

\begin{figure}[htbp]
	\centering
	\includegraphics[scale=0.7]{figures/svm_linear_leave_one_out_emb_200.png}
	\caption{SVM model with BoE(200) for each travel mode}
	\label{fig:svm_leave}
\end{figure}

The best result of the \emph{leave-one-group-out} was the Linear SVM model, with the dimensionality of 200 in the size of the feature vectors. Figure~\ref{fig:svm_leave} presents the results of each experiment led for the different hidden travel-mode classes. An interesting point to observe is the low performance obtained to the experiment with the travel-mode class "Walk" hidden. This is due to the different semantic and syntactic contexts that the word \emph{walk} is used. Although all other classes can be used in the same context, for example, \emph{car}, \emph{train}, or \emph{bus}, usually the word \emph{walk} is not applied in the same way.

Having the experiments concluded, we used the best model, in this case, Linear SVM for the dimensionality of 200, to predict the 4M tweets that composed the NYC dataset. Almost 300,000 tweets were classified as travel-related. After the classification step, a sample of 10,000 tweets was taken from all the travel-related classified tweets and it was produced a heat-map distribution in order to verify which are the most concentrated zones. Such distribution enables the identification of associations with metro, train, bus stations. In Figure~\ref{fig:brooklyn}, that shows the south of the Manhattan island and also the Brooklyn bridge, it is possible no note some agglomerations over the bridge and also in the port and closed to the Wall Street(4.5) where there are some metro stations. The Central Park is one place that also took our attention since presented several agglomerations of tweets. In this particular place, tweets related to the walk class were correctly identified.

\begin{table}[htbp]
	\centering
	\small
	\caption{Sample of tweet messages correctly classified}
	\label{tab:tweets_examples}
	\resizebox{\textwidth}{!}{\begin{tabular}{|c|}
			\hline
			when you get into your uber and he has a pipe in the back \\
			a ground stop for \#ewr is no longer in effect \#flightdelay \\
			snowy walk to work. \#blizzard2017 \#centralpark \#noreaster2017 \@ bethesda terrace fountain -  \textbf{Figure~\ref{fig:central_park}} \\
			m.t.a. n.y.c subways: w train irregular subway service at whitehall street-south ferry \#traffic - \textbf{Figure~\ref{fig:brooklyn}} \\ \hline
		\end{tabular}}
	\end{table}

\begin{figure}[htbp]
	\centering
	\begin{subfigure}[htbp]{0.7\textwidth}
		\centering
		\includegraphics[width=0.9\columnwidth]{figures/nyc_map.png}
		\caption{}
		\label{fig:brooklyn}
	\end{subfigure}
	
	\medskip
	
	\centering
	\begin{subfigure}[htbp]{0.7\textwidth}
		\centering
		\includegraphics[width=0.9\columnwidth]{figures/nyc_map2.png}
		\caption{}
		\label{fig:central_park}
	\end{subfigure}
	
	\caption[Spatial density of the predicted tweets]{Spatial density of the travel-related predicted tweets in New York City: (a) South of Manhattan and over the Brooklyn Bridge, (b) Central Park}
	\label{fig:nyc__geographical_distribution}
\end{figure}

\subsection{Concluding Remarks}
The main objective of this experiment was to devise a travel-related tweet classifier using word embeddings trained with geo-located English-speaking tweets. Similar to the Portuguese travel-related classification, we tried to build our model using a combined approach relying on bag-of-words and bag-of-embeddings features; however, results presented signs of dependency in the bag-of-words features which is not desired when facing real-world scenarios and lots of changes happen in short periods of time. On the other hand, by looking at the results, the almost perfect performance lead us to doubt about the existence of overfitting, and so, a \emph{leave-one-group-out} strategy was applied to validate the robustness of features. There, we excluded one of the travel-modes classes, which resulted in the fact that models using bag-of-words features could not maintain the performance previously demonstrated. Comparatively to the approach based on bag-of-words, the models using bag-of-embeddings features revealed consistency and robustness in the classification task. The Linear SVM model proved to be the best option with respect to the performance metrics considered in this work. We thus used that model trained with bag-of-embeddings to predict all the travel-related English tweets from our NYC dataset, whose results showed significant improvement over a standard bag-of-words baseline. Finally, we applied the resulting classifier to a stream of geo-located tweets in New York City, which was able to depict important spatio-temporal patterns.

\section{Summary}
This chapter has the purpose of report the experiments conducted over the period of this dissertation in order to help and validate the implementation of the different modules designed in our framework architecture.

Firstly, topic modelling techniques were applied under Portuguese-speaking tweets for two different \textit{megacities}, Rio de Janeiro and São Paulo, in order to extract information that may enabling interesting characterizations in different regions/zones of the cities regarding temporal and geographical distributions. Although huge restrictions regardind the labeling of each topic, results show promising contributions and informations to the \textit{smart cities} entities, allowing until this point possible identifications of the most \textit{hot} topics through time. The location of these topics is hard since, as it was mentioned in Section~\ref{sec:geographical_distribution}, the design of a geographical distribution is difficult because the majority of tweets do not have precisely location. In the future, this problem will need to be tackled in order to allow the cities' services possible geographical recognition of the topics.

Moreover, two different classification models for travel-related tweets were developed taking into consideration two possible languages in texts, Portuguese and English. Under the implementation of the Portuguese classification, we were able to prove that the combination of conventional techniques (bag-of-words) and recent ones (word embeddings) performed very well. However, for the English-speaking messages, where polysemy levels are higher than Portuguese-speaking messages, the same group of features did not perform well. Furthermore, by following a \textit{leave-one-group-out} strategy, we study and proved robustness regarding word-ebeddings features. The omission of a transport-mode class cause the model fed with bag-of-words features to perform worst than the one using only bag-of-embeddings. Such results need to be seen as a positive point since through this experiment we were able to capable of produce two classification models with higher levels of generalization. The resulting models were used in the development of our frameworks' travel-related classification module. We can conclude that is now able of discriminate travel-related tweets for two distinct speaking languages based on two important factors: consistency and robustness.